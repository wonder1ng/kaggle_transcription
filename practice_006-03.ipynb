{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "[[이유한님] 캐글 코리아 캐글 스터디 커널 커리큘럼](https://kaggle-kr.tistory.com/32)\n",
    "\n",
    "[TensorFlow Speech Recognition Challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge)\n",
    "\n",
    "[WavCeption V1: a 1-D Inception approach (LB 0.76)](https://www.kaggle.com/code/ivallesp/wavception-v1-a-1-d-inception-approach-lb-0-76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WavCeption V1: just a 1-D Inception approach\n",
    "\n",
    "I just wanted to share a little toy I have been playing with and gave me **amazing results**. As I currently don't have time, I would like to share it to see how people plays with it :-D. **The WavCeption V1** network seems to produce impressive results compared to a regular convolutional neural network, but in this competition it seems that there is a hard-work on the pre-processing and unknown tracks management. It is based on the Google's inception network, the same idea.\n",
    "\n",
    "I wrote some weeks ago a module implementing it so that it is easy to build an 1D-inception network by connecting lots of these modules in cascade (as you will see below).\n",
    "\n",
    "Unfortunately and due to several Kaggle constraints, it won't run in the kernel machine, so I encourage you to download it and run it in your own machine.\n",
    "\n",
    "By running the model for 12h without struggling too much I achieved 0.76 in the leaderboard (with 0.84 in local test). Some other trials in the same line gave me 0.89 in local, so there is a huge improvement in how you deal with the unknown clips :-D\n",
    "\n",
    "# DeepL 번역\n",
    "제가 가지고 놀다가 **놀라운 결과**를 얻은 작은 장난감을 공유하고 싶었습니다. 지금은 시간이 없어서 사람들이 어떻게 가지고 노는지 보기 위해 공유하고 싶습니다 :-D. **WavCeption V1** 네트워크는 일반 컨볼루션 신경망에 비해 인상적인 결과를 내는 것 같은데, 이번 대회에서는 전처리와 미지의 트랙 관리에 공을 들인 것 같습니다. 구글의 인셉션 네트워크와 같은 아이디어를 기반으로 하고 있습니다.\n",
    "\n",
    "저는 몇 주 전에 이를 구현하는 모듈을 작성했는데, 아래에서 보시는 것처럼 이 모듈을 많이 계단식으로 연결하면 1D-인셉션 네트워크를 쉽게 구축할 수 있습니다.\n",
    "\n",
    "안타깝게도 몇 가지 Kaggle 제약 조건으로 인해 커널 머신에서는 실행되지 않으므로 직접 다운로드하여 자신의 머신에서 실행해 보시기 바랍니다.\n",
    "\n",
    "큰 어려움 없이 12시간 동안 모델을 실행하여 리더보드에서 0.76을 달성했습니다(로컬 테스트에서는 0.84). 같은 라인의 다른 테스트에서는 로컬에서 0.89를 기록했으니, 알 수 없는 클립을 처리하는 방법이 크게 개선되었습니다 :-D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')    # 매직명렁어 안 될 때\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import IPython\n",
    "from numpy.fft import rfft, irfft\n",
    "# import numpy as np\n",
    "# import  random\n",
    "import itertools\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import  tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise generation functions\n",
    "\n",
    "The code in this section has been borrowed and adapted from:  \n",
    "https://github.com/python-acoustics/python-acoustics/blob/master/acoustics/generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms(x):\n",
    "    \"\"\"Mean value of signal 'x' squared.\n",
    "    :param x: Dynamic quantity.\n",
    "    :returns: Mean squared of 'x'.\n",
    "    \"\"\"\n",
    "    return (np.abs(x)**2.0).mean()\n",
    "\n",
    "def normalize(y, x=None):\n",
    "    \"\"\"normalize power in y to a (standard normal) white noise signal.\n",
    "    Optionally normalize to power in signal `x`.\n",
    "    # The mean power of a Gaussian with :math: `\\\\mu=0` and :math:`\\\\sigma=1` is 1.\n",
    "    \"\"\"\n",
    "    # return y * np.sqrt( (np.abs(x)**2.0).mean() / (np.abs(y)**2.0).mean() )\n",
    "    if x is not None:\n",
    "        x = ms(x)\n",
    "    else:\n",
    "        x = 1.0\n",
    "    return y * np.sqrt( x / ms(y) )\n",
    "    # return  y * np.sqrt( 1.0 / np.abs(y)**2.0).mean() )\n",
    "\n",
    "def white_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    return state.randn(N)\n",
    "\n",
    "def pink_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N % 2\n",
    "    X = state.randn(N // 2 + 1 + uneven) + 1j * state.randn(N // 2 + 1 + uneven)\n",
    "    S = np.sqrt(np.arange(len(X)) + 1.) # `+ 1` to avoid divide by zero\n",
    "    y = (irfft(X / S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)\n",
    "\n",
    "def blue_noise(N, state=None):\n",
    "    \"\"\"\n",
    "    Blue noise\n",
    "    \n",
    "    :param N: Amount of samples\n",
    "    :param state: State of PRNG.\n",
    "    :type state: :class: `np.random.RandomState`\n",
    "\n",
    "    Power increases with 6 dB per octave.\n",
    "    Power density increases with 3 dB per octave.\n",
    "\n",
    "    \"\"\"\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N % 2\n",
    "    X = state.randn(N // 2 + 1 + uneven) + 1j * state.randn(N // 2 + 1 + uneven)\n",
    "    S = np.sqrt(np.arange(len(X)))  # Filter\n",
    "    y = (irfft(X * S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)\n",
    "\n",
    "\n",
    "def brown_noise(N, state=None):\n",
    "    # 함수와 독스의 이름이 다름. 내용이 다음 함수와 다른 걸로 봐선 이름만 잘못된 듯\n",
    "    \"\"\"\n",
    "    Violet noise\n",
    "    \n",
    "    :param N: Amount of samples\n",
    "    :param state: State of PRNG.\n",
    "    :type state: :class: `np.random.RandomState`\n",
    "\n",
    "    Power decreases with -3 dB per octave.\n",
    "    Power density decreases with 6 dB per octave.\n",
    "\n",
    "    \"\"\"\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N % 2\n",
    "    X = state.randn(N // 2 + 1 + uneven) + 1j * state.randn(N // 2 + 1 + uneven)\n",
    "    S = np.sqrt(np.arange(len(X)) + 1)  # Filter\n",
    "    y = (irfft(X / S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)\n",
    "\n",
    "def violet_noise(N, state=None):\n",
    "    \"\"\"\n",
    "    Violet noise. Power increases with 6 dB per octave.\n",
    "    \n",
    "    :param N: Amount of samples\n",
    "    :param state: State of PRNG.\n",
    "    :type state: :class: `np.random.RandomState`\n",
    "\n",
    "    Power iecreases with +9 dB per octave.\n",
    "    Power density increases with +6 dB per octave.\n",
    "\n",
    "    \"\"\"\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N % 2\n",
    "    X = state.randn(N // 2 + 1 + uneven) + 1j * state.randn(N // 2 + 1 + uneven)\n",
    "    S = np.arange(len(X)) + 1 # Filter\n",
    "    y = (irfft(X * S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow utilities\n",
    "\n",
    "Utilities to modularize tensorflow common actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf Utils\n",
    "def get_tensorflow_configuration(device=\"0\", memory_fraction=1):\n",
    "    \"\"\"\n",
    "    Function for selecting the GPU to use and the amount of memory the process is allowed to use\n",
    "    :param device: which device should be used (str)\n",
    "    :param memory_fraction: which proportion of memory must be allocated (float)\n",
    "    :return: config to be passed to the session (tf object)\n",
    "    \"\"\"\n",
    "    device = str(device)\n",
    "    config = tf.ConfigProto()\n",
    "    config.allow_soft_placement = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = memory_fraction\n",
    "    config.gpu_options.visible_divice_list = device\n",
    "    return(config)\n",
    "\n",
    "def start_tensorflow_session(device=\"0\", memory_fraction=1):\n",
    "    \"\"\"\n",
    "    Starts a tensorflow session taking care of what GPU device is going to be used and\n",
    "    which is the fraction of memory that is going to be pre-allocated.\n",
    "    :device: string with the device number (str)\n",
    "    :memory_fraction: fraction of memory that is going to be pre-allocated in the specified\n",
    "    device (float [0, 1])\n",
    "    :return: configured tf.Session\n",
    "    \"\"\"\n",
    "    return(tf.Session(config=get_tensorflow_configuration(device=device, memory_fraction=memory_fraction)))\n",
    "\n",
    "def get_summary_writer(session, logs_path, project_id, version_id):\n",
    "    \"\"\"\n",
    "    For Tensorboard reporting\n",
    "    :param session: opened tensorflow session (tf.Session)\n",
    "    :param logs_path: path where tensorboard is looking for logs (str)\n",
    "    :param project_id: name of the project for reporting purpose (str)\n",
    "    :param version_id: name of the version for reporting purpose (str)\n",
    "    :return summary_writer: the tensorboard writer\n",
    "    \"\"\"\n",
    "    path = os.path.join(logs_path, \"{}_{}\".format(project_id, version_id))\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    summary_writer = tf.summary.FileWriter(path, graph_def=session.graph_def)\n",
    "    return(summary_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths management module\n",
    "Modules to deal with the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common paths\n",
    "def _norm_path(path):\n",
    "    \"\"\"\n",
    "    Decorator function intended for using it to normalize a the output of a path retrieval function. Useful for\n",
    "    fixing the slash/backslash windows cases.\n",
    "    \"\"\"\n",
    "    def normalize_path(*arg, **kwargs):\n",
    "        return os.path.normpath(path(*arg, **kwargs))\n",
    "    return normalize_path\n",
    "\n",
    "def _assure_path_exists(path):\n",
    "    \"\"\"\n",
    "    Decorator function intended for checking the existence of a the output of a path retrieva; function. Useful for\n",
    "    fixing the slash/backslash windows cases\n",
    "    \"\"\"\n",
    "    def assure_exists(*args, **kwargs):\n",
    "        p = path(*args, **kwargs)\n",
    "        assert os.path.exists(p), \"the following path does not exist: '{}'\".format(p)\n",
    "        return p\n",
    "    return assure_exists\n",
    "\n",
    "def _is_output_path(path):\n",
    "    \"\"\"\n",
    "    Decorator function intended for grouping the functions which are applied over the output of an output path retrieval\n",
    "    function\n",
    "    \"\"\"\n",
    "    @_norm_path\n",
    "    @_assure_path_exists\n",
    "    def check_existence_or_crate_it(*args, **kwargs):\n",
    "        if not os.path.exists(path(*args, **kwargs)):\n",
    "            \"Path does not exist... creating it: {}\".format(path(*args, **kwargs))\n",
    "            os.makedirs(path(*args, **kwargs))\n",
    "        return path(*args, **kwargs)\n",
    "    return check_existence_or_crate_it\n",
    "\n",
    "def _is_input_path(path):\n",
    "    \"\"\"\n",
    "    Decorator function intended for grouping the functions which are applied over the output of an input path retrieval\n",
    "    function\n",
    "    \"\"\"\n",
    "    @_norm_path\n",
    "    @_assure_path_exists\n",
    "    def check_existence(*args, **kwargs):\n",
    "        return path(*args, **kwargs)\n",
    "    return check_existence\n",
    "\n",
    "@_is_input_path\n",
    "def get_train_path():\n",
    "    path = \"./input/006_tensorflow-speech-recognition-challenge/train\"\n",
    "    return path\n",
    "\n",
    "@_is_input_path\n",
    "def get_test_path():\n",
    "    path = \"./input/006_tensorflow-speech-recognition-challenge/test\"\n",
    "    return path\n",
    "\n",
    "@_is_input_path\n",
    "def get_train_audio_path():\n",
    "    path = os.path.join(get_train_path(), \"audio\")\n",
    "    return path\n",
    "\n",
    "@_is_input_path\n",
    "def get_scoring_audio_path():\n",
    "    path = os.path.join(get_test_path(), \"audio\")\n",
    "    return path\n",
    "\n",
    "@_is_output_path\n",
    "def get_submission_path():\n",
    "    path = \"./working/output\"\n",
    "    return path\n",
    "\n",
    "@_is_output_path\n",
    "def get_silence_path():\n",
    "    path = \"./working/silence\"\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "Common general-purpose utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def batching(iterable, n = 1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx: min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Tools\n",
    "\n",
    "Data handling tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data tools\n",
    "def read_wav(filepath, pad=True):\n",
    "    \"\"\"\n",
    "    Given the filepath of a wav file, this function reads it, normalizes it and pads\n",
    "    it to assure it has 16ks samples.\n",
    "    :param filepath: existing filepath of a wav file (str)\n",
    "    :param pad: is padding required? (bool)\n",
    "    :returns: the sample and the target variable (tuple of (np.array, str))\n",
    "    \"\"\"\n",
    "    saomple_rate, x = wavfile.read(filepath)\n",
    "    target = os.path.split(os.path.split(filepath)[0])[1]\n",
    "    assert saomple_rate == 16000\n",
    "    if pad:\n",
    "        return np.pad(x, (0, 16000 - len(x)), mode=\"constant\") / 32768, target\n",
    "    else:\n",
    "        return x/32768, target\n",
    "\n",
    "def get_batcher(list_of_paths, batch_size, label_encoder=None, scoring=False):\n",
    "    \"\"\"\n",
    "    Builds a batch generator given a list of batches\n",
    "    :param list_of_paths: list of tuples with elements of format (filepath, target) (list)\n",
    "    :param batch_size: size of the batch (int)\n",
    "    :param label_encoder: fitted LabelEncoder (sklearn.LabelEnocoder | optional)\n",
    "    :returns: batch generator\n",
    "    \"\"\"\n",
    "    for filepaths in batching(list_of_paths, batch_size):\n",
    "        wavs, targets = zip(*list(map(read_wav, filepaths)))\n",
    "        if scoring:\n",
    "            yield np.expand_dims(np.row_stack(wavs), 2), filepaths\n",
    "        else:\n",
    "            if label_encoder is None:\n",
    "                yield np.expand_dims(np.row_stack(wavs), 2), np.row_stack(targets)\n",
    "            else:\n",
    "                yield np.expand_dims(np.row_stack(wavs), 2), np.expand_dims(label_encoder.transform(np.squeeze(targets)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture building blocks\n",
    "\n",
    "Inception-1D (a.k.a wavception) is a module I designed some weeks ago for this problem. It substantially enhances the performance of a regular convolutional neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(object):\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.999, name=\"batch_norm\"):\n",
    "        # with tf.variable_scope(name):\n",
    "        # with tf.compat.v1.variable_scope (name): # 1버전 함수 사용\n",
    "        with tf.name_scope(name): # 함수 변경\n",
    "            self.epsilon = epsilon\n",
    "            self.momentum = momentum\n",
    "            self.name =name\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        return tf.contrib.layers.batch_norm(x,\n",
    "                                            decay=self.momentum,\n",
    "                                            updates_collections=None,\n",
    "                                            epsilon=self.epsilon,\n",
    "                                            scale=True,\n",
    "                                            is_training=train,\n",
    "                                            scope=self.name)\n",
    "\n",
    "def inception_1d(x, is_train, depth, norm_function, activ_function, name):\n",
    "    \"\"\"Inception 1D module implementation.\n",
    "    :param x: input to current module (4D tensor with channels-last)\n",
    "    :param is_train: it is intented to be a boolean placeholder for controling the BatchNormalization behavior (0D tensor)\n",
    "    :param depth: linearly controls the depth of the network (int)\n",
    "    :param norm_function: normalization class (same format as the BatchNorm class above)\n",
    "    :param activ_function: tensorflow activation function (e.g. tf.nn.relu)\n",
    "    :param name: of the variable scpoe (str)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        x_norm = norm_function(name=\"norm_input\")(x, train-is_train)\n",
    "\n",
    "        # Branch 1: 64 x copy 1x1\n",
    "        branch_conv_1_1 = tf.layers.conv1d(inputs=x_norm, filters=16*depth, kernel_size=1,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_1_1\")\n",
    "        branch_conv_1_1 = norm_function(name=\"norm_conv_1_1\")(branch_conv_1_1, train=is_train)\n",
    "        branch_conv_1_1 = activ_function(branch_conv_1_1, \"activation_1_1\")\n",
    "\n",
    "        # Branch 2: 128 x conv 3x3\n",
    "        branch_conv_3_3 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_3_3_1\")\n",
    "        branch_conv_3_3 = norm_function(name=\"norm_conv_3_3_1\")(branch_conv_3_3, train=is_train)\n",
    "        branch_conv_3_3 = activ_function(branch_conv_3_3, \"activation_3_3_1\")\n",
    "        \n",
    "        branch_conv_3_3 = tf.layers.conv1d(inputs=branch_conv_3_3, filters=32*depth, kernel_size=3,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_3_3_2\")\n",
    "        branch_conv_3_3 = norm_function(name=\"norm_conv_3_3_2\")(branch_conv_3_3, train=is_train)\n",
    "        branch_conv_3_3 = activ_function(branch_conv_3_3, \"activation_3_3_2\")\n",
    "\n",
    "        # Branch 3: 128 x conv 5x5\n",
    "        branch_conv_5_5 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_5_5_1\")\n",
    "        branch_conv_5_5 = norm_function(name=\"norm_conv_5_5_1\")(branch_conv_5_5, train=is_train)\n",
    "        branch_conv_5_5 = activ_function(branch_conv_5_5, \"activation_5_5_1\")\n",
    "        \n",
    "        branch_conv_5_5 = tf.layers.conv1d(inputs=branch_conv_5_5, filters=32*depth, kernel_size=5,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_5_5_2\")\n",
    "        branch_conv_5_5 = norm_function(name=\"norm_conv_5_5_2\")(branch_conv_5_5, train=is_train)\n",
    "        branch_conv_5_5 = activ_function(branch_conv_5_5, \"activation_5_5_2\")\n",
    "\n",
    "        # Branch 4: 128 x conv 7x7\n",
    "        branch_conv_7_7 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_7_7_1\")\n",
    "        branch_conv_7_7 = norm_function(name=\"norm_conv_7_7_1\")(branch_conv_7_7, train=is_train)\n",
    "        branch_conv_7_7 = activ_function(branch_conv_7_7, \"activation_7_7_1\")\n",
    "        \n",
    "        branch_conv_7_7 = tf.layers.conv1d(inputs=branch_conv_7_7, filters=32*depth, kernel_size=7,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_7_7_2\")\n",
    "        branch_conv_7_7 = norm_function(name=\"norm_conv_7_7_2\")(branch_conv_7_7, train=is_train)\n",
    "        branch_conv_7_7 = activ_function(branch_conv_7_7, \"activation_7_7_2\")\n",
    "\n",
    "        # Branch 5: 16 x (max_pool 3x3 + conv 1x1)\n",
    "        branch_maxpool_3_3 = tf.layers.max_pooling1d(inputs=x_norm, pool_size=3, strides=1, padding=\"same\", name=\"maxpool_3\")\n",
    "        branch_maxpool_3_3 = norm_function(name=\"norm_maxpool_3_3\")(branch_maxpool_3_3, train=is_train)\n",
    "        branch_maxpool_3_3 = tf.layers.conv1d(inputs=branch_maxpool_3_3, filters=16, kernel_size=1,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_maxpool_3\")\n",
    "\n",
    "        # Branch 6: 16 x (max_pool 5x5 + conv 1x1)\n",
    "        branch_maxpool_5_5 = tf.layers.max_pooling1d(inputs=x_norm, pool_size=5, strides=1, padding=\"same\", name=\"maxpool_5\")\n",
    "        branch_maxpool_5_5 = norm_function(name=\"norm_maxpool_5_5\")(branch_maxpool_5_5, train=is_train)\n",
    "        branch_maxpool_5_5 = tf.layers.conv1d(inputs=branch_maxpool_5_5, filters=16, kernel_size=1,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_maxpool_5\")\n",
    "\n",
    "        # Branch 7: 16 x (max_pool 3x3 + conv 1x1)\n",
    "        branch_avgpool_3_3 = tf.layers.average_pooling1d(inputs=x_norm, pool_size=3, strides=1, padding=\"same\", name=\"avgpool_3\")\n",
    "        branch_avgpool_3_3 = norm_function(name=\"norm_avgpool_3_3\")(branch_avgpool_3_3, train=is_train)\n",
    "        branch_avgpool_3_3 = tf.layers.conv1d(inputs=branch_avgpool_3_3, filters=16, kernel_size=1,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_avgpool_3\")\n",
    "\n",
    "        # Branch 8: 16 x (max_pool 5x5 + conv 1x1)\n",
    "        branch_avgpool_5_5 = tf.layers.average_pooling1d(inputs=x_norm, pool_size=5, strides=1, padding=\"same\", name=\"avgpool_5\")\n",
    "        branch_avgpool_5_5 = norm_function(name=\"norm_avgpool_5_5\")(branch_avgpool_5_5, train=is_train)\n",
    "        branch_avgpool_5_5 = tf.layers.conv1d(inputs=branch_avgpool_5_5, filters=16, kernel_size=1,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_avgpool_5\")\n",
    "        \n",
    "        # Concatenate\n",
    "        output = tf.concat([branch_conv_1_1, branch_conv_3_3, branch_conv_5_5, branch_conv_7_7, branch_maxpool_3_3,\n",
    "                            branch_maxpool_5_5, branch_avgpool_3_3, branch_avgpool_5_5], axis=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic and provided noise addition\n",
    "filepaths_noise = glob.glob(os.path.join(get_train_audio_path(), \"_background_noise_\", \"*.wav\"))\n",
    "\n",
    "noise = np.concatenate(list(map(lambda x: read_wav(x, False)[0], filepaths_noise)))\n",
    "noise = np.concatenate([noise, noise[::-1]])\n",
    "synthetic_noise = np.concatenate([white_noise(N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  blue_noise(N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  pink_noise(N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  brown_noise(N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  violet_noise(N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  np.zeros(16000*60)])\n",
    "synthetic_noise //= np.max(np.abs(synthetic_noise))\n",
    "synthetic_noise = np.concatenate([synthetic_noise, (synthetic_noise+synthetic_noise[::-1])/2])\n",
    "all_noise = np.concatenate([noise, synthetic_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:11<00:00, 693.75it/s] \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(655321)\n",
    "random.seed(655321)\n",
    "\n",
    "path = get_silence_path()\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path) # It fails in kaggle kernel due to the read-only filesystem\n",
    "\n",
    "for noise_clip_no in tqdm(range(8000)):\n",
    "    if noise_clip_no <= 4000:\n",
    "        idx = np.random.randint(0, len(noise) - 16000)\n",
    "        clip = noise[idx: (idx + 16000)]\n",
    "    else:\n",
    "        idx = np.random.randint(0, len(synthetic_noise) - 16000)\n",
    "        clip = synthetic_noise[idx: (idx + 16000)]\n",
    "    wavfile.write(os.path.join(path, \"{0:04d}.wav\".format(noise_clip_no)), 16000,\n",
    "                  ((32767 * clip / np.max(np.abs(clip))).astype(np.int16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = glob.glob(os.path.join(get_train_audio_path(), \"**/*.wav\"), recursive=True)\n",
    "filepaths += glob.glob(os.path.join(get_silence_path(), \"**/*.wav\"), recursive=True)\n",
    "filepaths = list(filter(lambda fp: \"_background_noise_\" not in fp, filepaths))\n",
    "validation_list = open(os.path.join(get_train_path(), \"validation_list.txt\")).readlines()\n",
    "test_list = open(os.path.join(get_train_path(), \"testing_list.txt\")).readlines()\n",
    "validation_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), validation_list))\n",
    "testing_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), test_list))\n",
    "training_list = np.setdiff1d(filepaths, validation_list + testing_list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = glob.glob(os.path.join(get_train_audio_path(), \"**/*.wav\"), recursive=True)\n",
    "filepaths += glob.glob(os.path.join(get_silence_path(), \"**/*.wav\"), recursive=True)\n",
    "filepaths = list(filter(lambda fp: \"_background_noise_\" not in fp, filepaths))\n",
    "validation_list = open(os.path.join(get_train_path(), \"validation_list.txt\")).readlines()\n",
    "test_list = open(os.path.join(get_train_path(), \"testing_list.txt\")).readlines()\n",
    "validation_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), validation_list))\n",
    "testing_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), test_list))\n",
    "training_list = np.setdiff1d(filepaths, validation_list + testing_list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(655321)\n",
    "random.shuffle(filepaths)\n",
    "random.shuffle(validation_list)\n",
    "random.shuffle(test_list)\n",
    "random.shuffle(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 코드가 동일한데 assert 값이 달라 추후 확인 예정\n",
    "# Quick Unit-Tests\n",
    "# Test number of files and their consistencies\n",
    "assert all(map(lambda fp: os.path.splitext(fp)[1] == \".wav\", filepaths))\n",
    "assert len(filepaths) == 64727 - 6 + 8000\n",
    "# assert len(training_list) == len(filepaths) - 6798 - 6835\n",
    "# (72721, 59088)\n",
    "assert len(validation_list) == 6798\n",
    "assert len(testing_list) == 6835\n",
    "\n",
    "# Test file existence\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), validation_list))\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), testing_list))\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), training_list))\n",
    "# assert set(validation_list + testing_list + training_list) == set(filepaths)\n",
    "# 길이로 비교 시 (86354, 72721)\n",
    "\n",
    "# Test non-overlap among sets\n",
    "assert len(np.intersect1d(validation_list, testing_list)) == 0\n",
    "assert len(np.intersect1d(training_list, testing_list)) == 0\n",
    "assert len(np.intersect1d(training_list, validation_list)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes processing\n",
    "cardinal_calsses = list(set(map(lambda fp:os.path.split(os.path.split(fp)[0])[1], filepaths)))\n",
    "le_classes = LabelEncoder().fit(cardinal_calsses)\n",
    "Counter(map(lambda fp: os.path.split(os.path.split(fp)[0]), filepaths));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Unit-Tests\n",
    "# Test data preparation\n",
    "_gen_test = get_batcher(filepaths, 1000)\n",
    "batch_a_wav, batch_a_target = next(_gen_test)\n",
    "batch_b_wav, batch_b_target = next(_gen_test)\n",
    "_gen_test_le = get_batcher(filepaths, 1000, label_encoder=le_classes)\n",
    "batch_le_wav, batch_le_target = next(_gen_test_le)\n",
    "\n",
    "# Test batch matrix shape coherences\n",
    "assert batch_a_wav.shape == (1000, 16000, 1)\n",
    "assert batch_le_wav.shape == (1000, 16000, 1)\n",
    "assert batch_a_wav.shape == batch_b_wav.shape == batch_le_wav.shape\n",
    "\n",
    "# Test batch reproductibility\n",
    "assert np.sum(np.abs(batch_a_wav - batch_b_wav)) != 0\n",
    "assert len(batch_a_target) == len(batch_b_target) == len(batch_le_target)\n",
    "assert any(batch_a_target != batch_b_target)\n",
    "\n",
    "# Test classes label encoder\n",
    "assert all(batch_le_target == np.expand_dims(le_classes.transform(np.squeeze(batch_a_target)), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design\n",
    "\n",
    "Here it comes, WavCeption design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameSpacer:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "class Architecture:\n",
    "    def __init__(self, class_cardinality, seq_len=16000, name=\"architecture\"):\n",
    "        self.seq_len = seq_len\n",
    "        self.class_cardinality = class_cardinality\n",
    "        # self.optimizer = tf.train.Adamoptimizer(learning_rate=0.0001)\n",
    "        # v1 코드라 변경\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        self.name = name\n",
    "        self.define_computation_graph()\n",
    "\n",
    "        # Aliases\n",
    "        self.ph = self.placeholders\n",
    "        self.op = self.optimizers\n",
    "        self.summ = self.summaries\n",
    "    \n",
    "    def define_computation_graph(self):\n",
    "        # Reset graph\n",
    "        # tf.reset_default_graph() # v1 이슈\n",
    "        self.placeholders = NameSpacer(**self.define_placeholders())\n",
    "        self.core_model = NameSpacer(**self.define_core_model())\n",
    "        self.optimizer = NameSpacer(**self.define_optimizers())\n",
    "        self.summaries = NameSpacer(**self.define_summaries())\n",
    "    \n",
    "    def define_placeholders(self):\n",
    "        # with tf.variable_scope(\"Placeholders\"):\n",
    "        with tf.variable_creator_scope(\"Placeholders\"):\n",
    "            wav_in = tf.placeholder(dtype=tf.float32, shape=(None, self.seq_len, 1), name=\"wav_in\")\n",
    "            is_train = tf.placeholder(dtype=tf.bool, shape=None, name=\"is_train\")\n",
    "            target = tf.placeholder(dtype=tf.int32, shape=(None, 1), name=\"target\")\n",
    "            acc_dev = tf.placeholder(dtype=tf.float32, shape=None, name=\"acc_dev\")\n",
    "            loss_dev = tf.placeholder(dtype=tf.float32, shape=None, name=\"loss_dev\")\n",
    "            return ({\"wav_in\": wav_in, \"target\": target, \"is_train\": is_train, \"acc_dev\": acc_dev, \"loss_dev\": loss_dev})\n",
    "\n",
    "    def define_core_model(self):\n",
    "        # with tf.variable_scope(\"Core_Model\"):\n",
    "        with tf.variable_creator_scope(\"Core_Model\"):\n",
    "            x = inception_1d(x=self.placeholders.wav_in, is_train=self.placeholders.is_train,\n",
    "                             norm_function=BatchNorm, activ_function=tf.nn.relu, depth=1,\n",
    "                             name=\"Inception_1_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_1_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_2_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_2_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_2\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_3_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_3_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_3\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_4_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_4_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_4\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_5_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_5_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_5\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_6_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_6_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_6\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_7_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_7_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_7\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_8_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_8_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_8\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_9_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_9_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_9\")\n",
    "            x = tf.contrib.layers.flatten(x)\n",
    "            # x = tf.layers.flatten(x)\n",
    "            x = tf.layers.dense(BatchNorm(name=\"bn_dense_1\")(x, train=self.placeholders.is_train),\n",
    "                                128, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"dense_1\")\n",
    "            output = tf.layers.dense(BatchNorm(name=\"bn_dense_2\")(x, train=self.placeholders.is_train),\n",
    "                                self.class_cardinality, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"output\")\n",
    "        \n",
    "        return ({\"output\": output})\n",
    "\n",
    "    def define_losses(self):\n",
    "        with tf.variable_scope(\"Losses\"):\n",
    "            softmax_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(self.placeholders.target),\n",
    "                                                                        logists=self.core_model.output,\n",
    "                                                                        name=\"softmax\")\n",
    "            return ({\"softmax\": softmax_ce})\n",
    "    \n",
    "    def define_optimizers(self):\n",
    "        with tf.variable_scope(\"Optimization\"):\n",
    "            op = self.optimizer.minimize(self.losses.softmax)\n",
    "            \n",
    "            return ({\"op\": op})\n",
    "    \n",
    "    def define_summaries(self):\n",
    "        with tf.variable_scope(\"Summaries\"):\n",
    "            ind_max = tf.squeeze(tf.cast(tf.argmax(self.core_model.output, axis=1), tf.int32))\n",
    "            target = tf.squeeze(self.placeholders.target)\n",
    "            acc = tf.reduce_mean(tf.cast(tf.equal(ind_max, target), tf.float32))\n",
    "            loss = tf.reduce_mean(self.losses.softmax)\n",
    "            train_scalar_probes = {\"accuracy\": acc,\n",
    "                                   \"loss\": loss}\n",
    "            train_performance_scalar = [tf.summary.scalar(k, tf.reduce_mean(v), family=self.name)\n",
    "                                        for k, v in train_scalar_probes.items()]\n",
    "            train_performance_scalar = tf.summary.merge(train_performance_scalar)\n",
    "            dev_scalar_probes = {\"acc_dev\": self.placeholders.acc_dev,\n",
    "                                 \"loss_dev\": self.placeholders.loss_dev}\n",
    "            dev_performance_scalar = [tf.summary.scalar(k, v, family=self.name) for k, v in dev_scalar_probes.items()]\n",
    "            dev_performance_scalar = tf.summary.merge(dev_performance_scalar)\n",
    "            return ({\"accuracy\": acc, \"loss\": loss, \"s_tr\": train_performance_scalar, \"s_de\": dev_performance_scalar})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model\n",
    "\n",
    "You should use a GPU to run the model if you don't want it to take forever... In addition, you should decide when to stop the network to make the prediction. It took me 12h in a Titan X Pascal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Architecture(class_cardinality=len(cardinal_calsses), name=\"wavception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = start_tensorflow_session(device=\"1\")\n",
    "sw = get_summary_writer(sess, \"~/.logs_ensorboard/\", \"wavception\", \"V1\") # Adjust your tensorboard logs path here\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(655321)\n",
    "random.seed(655321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/ivallesp/wavception-v1-a-1-d-inception-approach-lb-0-76"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
