{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[이유한님] 캐글 코리아 캐글 스터디 커널 커리큘럼](https://kaggle-kr.tistory.com/32)\n",
    "\n",
    "[3rd level. Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk)\n",
    "\n",
    "[Stacking Test-Sklearn, XGBoost, CatBoost, LightGBM](https://www.kaggle.com/code/eliotbarr/stacking-test-sklearn-xgboost-catboost-lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Starter based on Allstate Faron's Script\n",
    "#https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\n",
    "# Preprocessing from ogrellier\n",
    "#https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from math import sqrt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import gc\n",
    "\n",
    "NFOLDS = 3\n",
    "SEED = 0\n",
    "NROWS = None\n",
    "\n",
    "data = pd.read_csv('./input/003_home-credit-default-risk/application_train.csv')\n",
    "test = pd.read_csv('./input/003_home-credit-default-risk/application_test.csv')\n",
    "prev = pd.read_csv('./input/003_home-credit-default-risk/previous_application.csv')\n",
    "\n",
    "# print('%-55s | %7s | %18s | %10s | %10s'\n",
    "#       % ('FEATURES', 'TYPE', 'NB VALUES', 'NB NaNS', 'NaNs (%)'))\n",
    "# for f_ in data: # .dtypes\n",
    "#     print('%-55s | %7s | %10s | %10s |      %5.2f'\n",
    "#           % (f_, str(data[f_].dtype),\n",
    "#              str(len(data[f_].value_counts(dropna=False))),\n",
    "#              str(data[f_].isnull().sum()),\n",
    "#              100 * data[f_].isnull().sum() / data.shape[0]\n",
    "#             )\n",
    "#          )\n",
    "\n",
    "categorical_feats = [\n",
    "    f for f in data.columns if data[f].dtype == 'object'\n",
    "]\n",
    "\n",
    "for f_ in categorical_feats:\n",
    "    data[f_], indexer = pd.factorize(data[f_])  # pd.factorize: 라벨 인코딩 값과 기존값.unique 반환\n",
    "    test[f_] = indexer.get_indexer(test[f_])    # index.get_indexer: 일치하는 값의 index_num 반환, 없는 값은 -1\n",
    "\n",
    "gc.enable()     # 자동 gc 활성화\n",
    "\n",
    "y_train = data['TARGET']\n",
    "del data['TARGET']\n",
    "\n",
    "###################################\n",
    "# PLEASE DON'T DO THIS AT HOME LOL\n",
    "# Averaging factorized categorical features defeats my own reasoning\n",
    "###################################\n",
    "prev_cat_features = [\n",
    "    f_ for f_ in prev.columns if prev[f_].dtype=='object'\n",
    "]\n",
    "for f_ in prev_cat_features:\n",
    "    prev[f_], _ = pd.factorize(prev[f_])\n",
    "\n",
    "avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "cnt_prev = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "avg_prev['nb_app'] = cnt_prev['SK_ID_PREV']\n",
    "del avg_prev['SK_ID_PREV']\n",
    "\n",
    "x_train = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "x_test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "x_train = x_train.fillna(0)\n",
    "x_test = x_test.fillna(0)\n",
    "\n",
    "ntrain =  x_train.shape[0]\n",
    "ntest =  x_test.shape[0]\n",
    "\n",
    "exclude_feats = ['SK_ID_CURR']\n",
    "features = [f_ for f_ in x_train.columns if f_ not in exclude_feats]\n",
    "\n",
    "x_train = x_train[features]\n",
    "x_test = x_test[features]\n",
    "\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_sate'] = seed\n",
    "        self.clf = clf(**params)\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]\n",
    "\n",
    "class CatboostWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_sate'] = seed\n",
    "        self.clf = clf(**params)\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]\n",
    "\n",
    "class LightGBMWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['feature_fraction_seed'] = seed\n",
    "        params['baggibg_seed'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]\n",
    "\n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 250)\n",
    "\n",
    "    def train(self , x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))\n",
    "\n",
    "def get_oof(clf):\n",
    "    oof_train = np.zeros((ntrain, ))\n",
    "    oof_test = np.zeros((ntest, ))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        x_tr = x_train.loc[train_index]\n",
    "        y_tr = y_train.loc[train_index]\n",
    "        x_te = x_train.loc[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "\n",
    "et_params = {\n",
    "    'n_jobs' : 16,\n",
    "    'n_estimators': 200,\n",
    "    'max_features': 0.5,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_leaf': 2,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
