{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[이유한님] 캐글 코리아 캐글 스터디 커널 커리큘럼](https://kaggle-kr.tistory.com/32)  \n",
    "\n",
    "[1st level. Costa Rican Household Poverty Level Prediction](https://www.kaggle.com/c/costa-rican-household-poverty-prediction)\n",
    "\n",
    "[XGBoost](https://www.kaggle.com/code/skooch/xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGMB with random split for early stopping\n",
    "__Edits by Eric Antoine Scuccimarra__ - This is a fork of https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro, by Misha Losvyi, with a few changes:\n",
    "\n",
    "- The LightGBM models have been replaced with XGBoost and the code has been updated accordingly.\n",
    "- I am also fitting VotingClassifiers of RandomForests and ensembling the results of the XGBs with the RFs.\n",
    "- Some additional features have been added.\n",
    "- Some features which were previously dropped have been retained.\n",
    "- Some of the code has been reorganized.\n",
    "- Rather than splitting the data once and using the validation data for the LGBM early stopping, I split the data during the training so the entire training set can be trained on. I found that this works better than a k-fold split in this case.\n",
    "\n",
    "Some additional features were taken from: https://www.kaggle.com/kuriyaman1002/reduce-features-140-84-keeping-f1-score, by Kuriyaman.\n",
    "\n",
    "__Notes from Original Kernel (edited by EAS)__:\n",
    "\n",
    "This kernel closely follows https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro, but instead of running hyperparameter optimisation it uses optimal values from that kernel and thus runs faster.\n",
    "\n",
    "Several key points:\n",
    "\n",
    "- __This kernel runs training on the heads of housholds only__ (after extracting aggregates over households). This follows the announced scoring startegy: Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored. (from the data description). However, at the moment it seems that evaluation depends also on non-head household members, see https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115. In practise, ful prediction gives ~0.4 PLB score, while replacing all non-head entries with class 1 leads to a drop down to ~0.2 PLB score\n",
    "- __It seems to be very important to balance class frequencies.__ Without balancing a trained model gives ~0.39 PLB / ~0.43 local test, while adding balancing leads to ~0.42 PLB / 0.47 local test. One can do it by hand, one can achieve it by undersampling. But the simplest (and more powerful compared to undersampling) is to set class_weight='balanced' in the LightGBM model constructor in sklearn API.\n",
    "- __This kernel uses macro F1 score to early stopping in training__. This is done to align with the scoring strategy.\n",
    "- Categoricals are turned into numbers with proper mapping instead of blind label encoding.\n",
    "- __OHE if reversed into label encoding, as it is easier to digest for a tree model.__ This trick would be harmful for non-tree models, so be careful.\n",
    "- __idhogar is NOT used in training.__ The only way it could have any info would be if there is a data leak. We are fighting with poverty here- exploiting leaks will not reduce poverty in any way :)\n",
    "- __There are aggregations done within households and new features are hand-crafted.__ Note, that there are not so many features that can be aggregated, as most are already quoted on household level.\n",
    "- __A voting classifier is used to average over several LightGBM models__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepL 번역\n",
    "__에릭 앙투안 스쿠치마라에 의해 수정됨__ - 이 포크는 미샤 로스비가 작성한 https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro 의 포크이며, 몇 가지 변경 사항이 있습니다:\n",
    "- LightGBM 모델이 XGBoost로 대체되었으며 그에 따라 코드가 업데이트되었습니다.\n",
    "- 또한 랜덤포레스트의 투표 분류기를 장착하고, XGB의 결과를 RF와 조합하고 있습니다.\n",
    "- 몇 가지 추가 기능이 추가되었습니다.\n",
    "- 이전에 삭제되었던 일부 기능은 그대로 유지되었습니다.\n",
    "- 일부 코드가 재구성되었습니다.\n",
    "- 데이터를 한 번 분할하여 LGBM 조기 정지를 위한 검증 데이터로 사용하는 대신 전체 훈련 세트를 훈련할 수 있도록 훈련 중에 데이터를 분할했습니다. 이 경우 K-배 분할보다 이 방법이 더 효과적이라는 것을 알게 되었습니다.\n",
    "\n",
    "몇 가지 추가 기능은 Kuriyaman의 https://www.kaggle.com/kuriyaman1002/reduce-features-140-84-keeping-f1-score 에서 가져왔습니다.\n",
    "\n",
    "__원본 커널의 참고 사항(EAS에서 편집)__:\n",
    "이 커널은 https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro 을 거의 따르지만 하이퍼파라미터 최적화를 실행하는 대신 해당 커널의 최적 값을 사용하므로 더 빠르게 실행됩니다.\n",
    "\n",
    "몇 가지 핵심 사항:\n",
    " \n",
    "- __이 커널은 세대주만 대상으로 훈련을 실행합니다__ (세대별 집계 추출 후). 이는 발표된 채점 시작 시점을 따릅니다: 채점에는 세대주만 사용된다는 점에 유의하세요. 모든 가구원은 시험 + 샘플 제출에 포함되지만, 가구주만 채점됩니다. (데이터 설명에서). 그러나 현재로서는 세대주가 아닌 가구원도 평가에 포함되는 것으로 보입니다(https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115 참조). 실제로 완전 예측은 약 0.4 PLB 점수를 주는 반면, 비가구원 항목을 모두 1등급으로 대체하면 약 0.2 PLB 점수로 떨어집니다.\n",
    "\n",
    "- __클래스 주파수의 균형을 맞추는 것이 매우 중요한 것 같습니다__. 훈련된 모델을 밸런싱하지 않으면 ~0.39 PLB / ~0.43 로컬 테스트가 나오는 반면 밸런싱 리드를 추가하면 ~0.42 PLB / 0.47 로컬 테스트가 나옵니다. 수작업으로 할 수도 있고 언더샘플링을 통해 달성할 수도 있습니다. 하지만 가장 간단하고 언더샘플링에 비해 더 강력한 방법은 sklearn API의 LightGBM 모델 생성자에서 class_weight='balanced'를 설정하는 것입니다.\n",
    "\n",
    "- __이 커널은 매크로 F1 점수를 사용하여 훈련에서 조기 중단합니다__. 이는 채점 전략에 맞추기 위해 수행됩니다.\n",
    "- 범주형은 블라인드 레이블 인코딩 대신 적절한 매핑을 통해 숫자로 변환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  #linear algebra\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "# from sklearn.externals.joblib import Parallel, delayed    # 업데이트 후 미지원\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following categorical mapping originates from [this kernel](https://www.kaggle.com/code/mlisovyi/categorical-variables-encoding-function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# this only transforms the idhogar field, the other things this function used to do are done elsewhere\n",
    "\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "\n",
    "# plot feature importance for sklearn decision trees\n",
    "def feature_importance(forest, X_train, display_results=True):\n",
    "    ranked_list = []\n",
    "    zero_features = []\n",
    "\n",
    "    importances = forest.feature_importances_\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    if display_results:\n",
    "        # Print the feature ranking\n",
    "        print(\"Feature ranking:\")\n",
    "    \n",
    "    for f in range(X_train.shape[1]):\n",
    "        if display_results:\n",
    "            print(\"%d. feature %d (%f)\" % (f +1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n",
    "        \n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "\n",
    "        if importances[indices[f]] == 0.0:\n",
    "            zero_features.append(X_train.columns[indices[f]])\n",
    "    \n",
    "    return ranked_list, zero_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also feature engineering magic happening here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'),\n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man)fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('humna_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2')\n",
    "                 ]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "    \n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)\n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "    \n",
    "    # aggregation rules over household\n",
    "    aggs_num = {'age': ['min', 'max', 'mean'],\n",
    "                'escolari': ['min', 'max', 'mean']\n",
    "                }\n",
    "    \n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean', 'count']\n",
    "    \n",
    "    # aggregation over household\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + '_' + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "    \n",
    "    # Drop id's\n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert one hot encoded fields to label encoding\n",
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu',\n",
    "               'epared', 'etecho', 'eviv', 'estadocivi;', 'parentesco',\n",
    "               'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        # deal with those OHE, where there is a sum over columns == 0\n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'.format(s_))\n",
    "            # dummy column name to be added\n",
    "            col_dummy = s_ + '_dummy'\n",
    "            # add the column to the dataframe\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            # add the name to the list of columns to be label-encoded\n",
    "            cols_s_.append(col_dummy)\n",
    "            # proof-check, that now the category is complete\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                print(\"The category completion did not work\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./input/004_costa-rican-household-poverty-prediction/train.csv')\n",
    "test = pd.read_csv('./input/004_costa-rican-household-poverty-prediction/test.csv')\n",
    "\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    # encode the idhogar\n",
    "    encode_data(df_)\n",
    "\n",
    "    # creaete aggregate features\n",
    "    return do_features(df_)\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up some missing data and convert objects to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some dependencies are Na, fill those with the squeare root of the square\n",
    "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
    "\n",
    "# fill \"no\"s for education with 0s\n",
    "train.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "train.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "test.loc[test['edjefa'] == \"no\", 'edjefa'] = 0\n",
    "test.loc[test['edjefe'] == \"no\", 'edjefe'] = 0\n",
    "\n",
    "# if education is \"yes\" and person is head of household, fill with escolari\n",
    "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] ==1), \"edjefa\"] = \\\n",
    "    train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] ==1), \"edjefe\"] = \\\n",
    "    train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] ==1), \"edjefa\"] = \\\n",
    "    test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] ==1), \"edjefe\"] = \\\n",
    "    test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "# this field is supposed to be interaction between gender and escolari, but it isn't clear what \"yes\" means, let's fill it with 4\n",
    "train.loc[train['edjefa'] == \"yes\", 'edjefa'] = 4\n",
    "train.loc[train['edjefe'] == \"yes\", 'edjefe'] = 4\n",
    "\n",
    "test.loc[test['edjefa'] == \"yes\", 'edjefa'] = 4\n",
    "test.loc[test['edjefe'] == \"yes\", 'edjefe'] = 4\n",
    "\n",
    "# convert to int for our models\n",
    "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
    "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
    "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
    "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
    "\n",
    "# create feature with max education of either head of household\n",
    "train['edjef'] = np.max(train[['edjefa', 'edjefe']], axis=1)\n",
    "test['edjef'] = np.max(test[['edjefa', 'edjefe']], axis=1)\n",
    "\n",
    "# fill some nas\n",
    "train['v2a1'] = train['v2a1'].fillna(0)\n",
    "test['v2a1'] = test['v2a1'].fillna(0)\n",
    "\n",
    "train['rez_esc'] = train['rez_esc'].fillna(0)\n",
    "test['rez_esc'] = test['rez_esc'].fillna(0)\n",
    "\n",
    "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "# fix some inconsistencies in the data - some rows indicate both that the household does and does not have a toilet,\n",
    "# if there is no water we'll assume they do not\n",
    "train.loc[(train.v14a == 1) & (train.sanitario1 == 1) & (train.abastaguano==0), 'v14a'] = 0\n",
    "train.loc[(train.v14a == 1) & (train.sanitario1 == 1) & (train.abastaguano==0), 'sanitario1'] = 0\n",
    "\n",
    "test.loc[(test.v14a == 1) & (test.sanitario1 == 1) & (test.abastaguano==0), 'v14a'] = 0\n",
    "test.loc[(test.v14a == 1) & (test.sanitario1 == 1) & (test.abastaguano==0), 'sanitario1'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "\n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_ = xx_func.iloc[:train_.shape[0], :].drop('Target', axis=1)\n",
    "\n",
    "    del xx, xx_func\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OHE in techo is incomplete. A new column will be added before label encoding\n",
      "The OHE in estadocivi; is incomplete. A new column will be added before label encoding\n",
      "The OHE in instlevel is incomplete. A new column will be added before label encoding\n",
      "The OHE in manual_elec is incomplete. A new column will be added before label encoding\n"
     ]
    }
   ],
   "source": [
    "# convert the one hot fields into label encoded\n",
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geo aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE',\n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency',\n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar'] + cols_nums)],\n",
    "                        pd.get_dummies(df_[cols_2_ohe],\n",
    "                                       columns=cols_2_ohe)], axis=1)\n",
    "    \n",
    "    geo_agg = tmp_df.groupby(['lugar_LE', 'idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n",
    "\n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "# add some aggregates by geography\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the number of people over 18 in each household\n",
    "train['num_over_18'] = 0\n",
    "# train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform('count')\n",
    "train.loc[train[train.age >= 18].index, 'num_over_18'] = train[train.age >= 18].groupby('idhogar').transform('count')\n",
    "train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "train['num_over_18'] = train['num_over_18'].fillna(0)\n",
    "\n",
    "test['num_over_18'] = 0\n",
    "# test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform('count')\n",
    "test.loc[test[test.age >= 18].index, 'num_over_18'] = test[test.age >= 18].groupby('idhogar').transform('count')\n",
    "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "test['num_over_18'] = test['num_over_18'].fillna(0)\n",
    "\n",
    "# add some extra features, these were taken from another kernel\n",
    "def extract_features(df):\n",
    "    df['bedrooms_torooms'] = df['bedrooms'] / df['rooms']\n",
    "    df['rent_to_rooms'] = df['v2a1'] / df['rooms']\n",
    "    df['tamhog_to_rooms'] = df['tamhog'] / df['rooms']  # tamhog - size of the household\n",
    "    df['r4t3_to_tamhog'] = df['r4t3'] / df['tamhog']    # r4t3 - Total persons in the household\n",
    "    df['r4t3_to_rooms'] = df['r4t3'] / df['rooms']      # r4t3 - Total persons in the household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1'] / df['r4t3']        # rent to people in household\n",
    "    # 위의 열을 아래의 값이 덮어쓰게 됨. 추후 확인\n",
    "    df['v2a1_to_r4t3'] = df['v2a1'] / (df['r4t3'] - df['r4t1']) # rent to people under age 12\n",
    "    df['hhsize_to_rooms'] = df['hhsize'] / df['rooms']  # rooms per person\n",
    "    df['rent_to_hhsize'] = df['v2a1'] / df['hhsize']    # rent to household size\n",
    "    df['rent_to_over_18'] = df['v2a1'] / df['num_over_18']\n",
    "    # some households have no one over 18, use the total rent for those\n",
    "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] =df[df.num_over_18 == 0].v2a1\n",
    "\n",
    "extract_features(train)\n",
    "extract_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplitcated columns\n",
    "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
    "                 'mobilephone', 'female']\n",
    "\n",
    "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n",
    "\n",
    "needless_cols.extend(instlevel_cols)\n",
    "\n",
    "train = train.drop(needless_cols, axis=1)\n",
    "test = test.drop(needless_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data\n",
    "We split the data by household to avoid leakage, since rows belonging to the same household usually have the same target. Since we filter the data to only include heads of household this isn't technically necessary, but it provides an easy way to use the entire training data set if we want to do that.\n",
    "\n",
    "Note that after splitting the data we overwrite the train data with the entire data set so we can train on all of the data. The split_data function does the same thing without overwriting the data, and is used within the training loop to (hopefully) approximate a K-Fold split.\n",
    "  \n",
    "# DeepL 번역\n",
    "같은 가구에 속한 행은 일반적으로 동일한 대상을 가지므로 누출을 방지하기 위해 데이터를 가구별로 분할합니다. 세대주만 포함하도록 데이터를 필터링하기 때문에 기술적으로 반드시 필요한 것은 아니지만, 그렇게 하려면 전체 학습 데이터 세트를 쉽게 사용할 수 있는 방법을 제공합니다.\n",
    "\n",
    "데이터를 분할한 후에는 전체 데이터 세트로 훈련 데이터를 덮어쓰므로 모든 데이터에 대해 훈련할 수 있습니다. split_data 함수는 데이터를 덮어쓰지 않고 동일한 작업을 수행하며, 훈련 루프 내에서 (희망적으로) K-Fold 분할의 근사치를 구하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(train, y, sample_weight=None, households=None, test_percentage=0.20, seed=None): # uncomment for extra randomness\n",
    "    # np.random.seed(seed=seed)\n",
    "    train2 = train.copy()\n",
    "\n",
    "    # pick some random households to use for the test data\n",
    "    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n",
    "\n",
    "    # select households which are in the random selection\n",
    "    cv_idx = np.isin(households, cv_hhs)\n",
    "    X_test = train2[cv_idx]\n",
    "    y_test = y[cv_idx]\n",
    "\n",
    "    X_train = train2[~cv_idx]\n",
    "    y_train = y[~cv_idx]\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        y_train_weights = sample_weight[~cv_idx]\n",
    "        return X_train, y_train, X_test, y_test, y_train_weights\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = train.copy()\n",
    "X = train.query('parentesco1==1')\n",
    "\n",
    "# oull out and drop the target variable\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'], axis=1)\n",
    "\n",
    "np.random.seed(seed=None)\n",
    "\n",
    "train2 = X.copy()\n",
    "\n",
    "train_hhs = train2.idhogar\n",
    "\n",
    "households = train2.idhogar.unique()\n",
    "cv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False)\n",
    "\n",
    "cv_idx = np.isin(train2.idhogar, cv_hhs)\n",
    "\n",
    "X_test = train2[cv_idx]\n",
    "y_test = y[cv_idx]\n",
    "\n",
    "X_train = train2[~cv_idx]\n",
    "y_train = y[~cv_idx]\n",
    "\n",
    "# train on entire dataset\n",
    "X_train = train2\n",
    "y_train = y\n",
    "\n",
    "train_households = X_train.idhogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out the class weights for training with unbalanced classes\n",
    "# y_ytain_weights = class_weight.compute_class_weight('balanced', y_train, indices=None)\n",
    "y_ytain_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some features which aren;t used by yhe LGBM or have very low importance\n",
    "extra_drop_features = [\n",
    "    'agg18_estadocivil1_MEAN',\n",
    "    'agg18_estadocivil6_COUNT',\n",
    "    'agg18_estadocivil7_COUNT',\n",
    "    'agg18_parentesco10_COUNT',\n",
    "    'agg18_parentesco11_COUNT',\n",
    "    'agg18_parentesco12_COUNT',\n",
    "    'agg18_parentesco1_COUNT',\n",
    "    'agg18_parentesco2_COUNT',\n",
    "    'agg18_parentesco3_COUNT',\n",
    "    'agg18_parentesco4_COUNT',\n",
    "    'agg18_parentesco5_COUNT',\n",
    "    'agg18_parentesco6_COUNT',\n",
    "    'agg18_parentesco7_COUNT',\n",
    "    'agg18_parentesco8_COUNT',\n",
    "    'agg18_parentesco9_COUNT',\n",
    "    'geo_elimbasu_LE_4',\n",
    "    'geo_energcocinar_LE_1',\n",
    "    'geo_energcocinar_LE_2',\n",
    "    'geo_epared_LE_0',\n",
    "    'geo_hogar_mayor',\n",
    "    'geo_manual_elec_LE_2',\n",
    "    'geo_pared_le_3',\n",
    "    'geo_pared_le_4',\n",
    "    'geo_pared_le_5',\n",
    "    'geo_pared_le_6',\n",
    "    'num_over_18',\n",
    "    'parentesco_LE',\n",
    "    'rez_esc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_drop_cols = extra_drop_features + ['idhogar', 'parentesco1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a voting classifier\n",
    "Define a derived VotingClassifier class to be able to pass fit_params for early stopping. Vote based on LGBM models with early stopping based on macro F1 and decaying learning rate.\n",
    "\n",
    "The parameters are optimised with a random search in this kernel: https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro\n",
    "  \n",
    "# DeepL 번역\n",
    "조기 정지를 위해 fit_params를 전달할 수 있도록 파생된 투표 분류기 클래스를 정의합니다. 매크로 F1과 학습률 감쇠에 따른 조기 정지를 사용하는 LGBM 모델을 기반으로 투표합니다.\n",
    "파라미터는 이 커널에서 무작위 검색으로 최적화됩니다: https://www.kaggle.com/code/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "opt_parameters = {'max_depth': 35, 'eta': 0.1, 'silent': 0,  'objective': 'multi: softmax', 'min_child_weight': 1, 'num_class': 4, 'gamma': 2.0, 'colsample_bylevel': 0.9, 'subsample': 0.84, 'colsample_bytree': 0.88, 'reg_lambda': 0.40}\n",
    "# 5\n",
    "opt_parameters = {'max_depth': 35, 'eta': 0.15, 'silent': 1,  'objective': 'multi: softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35}\n",
    "# 6\n",
    "# opt_parameters = {'max_depth': 35, 'eta': 0.15, 'silent': 0,  'objective': 'multi: softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.75, 'colsample_bylevel': 0.95, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35}\n",
    "# # 7\n",
    "# opt_parameters = {'max_depth': 35, 'eta': 0.12, 'silent': 0,  'objective': 'multi: softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 3.25, 'colsample_bylevel': 0.95, 'subsample': 0.88, 'colsample_bytree': 0.88, 'reg_lambda': 0.35}\n",
    "\n",
    "def evaluate_macroF1_lgb(predictions, truth):\n",
    "    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n",
    "    pred_labels = predictions.argmax(axis=1)\n",
    "    truth = truth.get_label()\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', 1-f1)\n",
    "\n",
    "fit_params = {\"early_stopping_rounds\": 500,\n",
    "              \"eval_metric\": evaluate_macroF1_lgb,\n",
    "              \"eval_set\": [(X_train, y_train), (X_test, y_test)],\n",
    "              'verbose': False\n",
    "              }\n",
    "\n",
    "def learning_rate_power_0907(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate * np.power(.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['verbose'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "    estimator = clone(estimator1)\n",
    "\n",
    "    # randomly split the data so we have a test set for early stopping\n",
    "    if sample_weight is not None:\n",
    "        X_train, y_train, X_test, y_test, y_ytain_weight = split_data(X, y, sample_weight, households=train_households)\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = split_data(X, y, None, households=train_households)\n",
    "\n",
    "    # update the fit params with our new split\n",
    "    fit_params['eval_set'] = [(X_test, y_test)]\n",
    "\n",
    "    # fit the estimator\n",
    "    if sample_weight is not None:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, sample_weight=y_ytain_weight, **fit_params)\n",
    "    else:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
    "    \n",
    "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
    "        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
    "    else:\n",
    "        best_train = f1_score(y_train, estimator.predict(X_train), average='macro')\n",
    "        best_cv = f1_score(y_test, estimator.predict(X_test), average='macro')\n",
    "        print('Train F1:', best_train)\n",
    "        print('Test F1:', best_cv)\n",
    "    \n",
    "    # reject some estimators based on their performance on train and test sets\n",
    "    if threshold:\n",
    "        # if the valid score is very high we'll allow a little more leeway with the train scores\n",
    "        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n",
    "            return estimator\n",
    "        \n",
    "        # else recurse until we get a better one\n",
    "        else:\n",
    "            print(\"Unacceptable!!! Trying again...\")\n",
    "            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n",
    "    \n",
    "    else:\n",
    "        return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/skooch/xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
