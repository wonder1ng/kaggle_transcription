{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[이유한님] 캐글 코리아 캐글 스터디 커널 커리큘럼](https://kaggle-kr.tistory.com/32)  \n",
    "\n",
    "[1st level. Costa Rican Household Poverty Level Prediction](https://www.kaggle.com/c/costa-rican-household-poverty-prediction)\n",
    "\n",
    "[XGBoost](https://www.kaggle.com/code/skooch/xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGMB with random split for early stopping\n",
    "__Edits by Eric Antoine Scuccimarra__ - This is a fork of https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro, by Misha Losvyi, with a few changes:\n",
    "\n",
    "- The LightGBM models have been replaced with XGBoost and the code has been updated accordingly.\n",
    "- I am also fitting VotingClassifiers of RandomForests and ensembling the results of the XGBs with the RFs.\n",
    "- Some additional features have been added.\n",
    "- Some features which were previously dropped have been retained.\n",
    "- Some of the code has been reorganized.\n",
    "- Rather than splitting the data once and using the validation data for the LGBM early stopping, I split the data during the training so the entire training set can be trained on. I found that this works better than a k-fold split in this case.\n",
    "\n",
    "Some additional features were taken from: https://www.kaggle.com/kuriyaman1002/reduce-features-140-84-keeping-f1-score, by Kuriyaman.\n",
    "\n",
    "__Notes from Original Kernel (edited by EAS)__:\n",
    "\n",
    "This kernel closely follows https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro, but instead of running hyperparameter optimisation it uses optimal values from that kernel and thus runs faster.\n",
    "\n",
    "Several key points:\n",
    "\n",
    "- __This kernel runs training on the heads of housholds only__ (after extracting aggregates over households). This follows the announced scoring startegy: Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored. (from the data description). However, at the moment it seems that evaluation depends also on non-head household members, see https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115. In practise, ful prediction gives ~0.4 PLB score, while replacing all non-head entries with class 1 leads to a drop down to ~0.2 PLB score\n",
    "- __It seems to be very important to balance class frequencies.__ Without balancing a trained model gives ~0.39 PLB / ~0.43 local test, while adding balancing leads to ~0.42 PLB / 0.47 local test. One can do it by hand, one can achieve it by undersampling. But the simplest (and more powerful compared to undersampling) is to set class_weight='balanced' in the LightGBM model constructor in sklearn API.\n",
    "- __This kernel uses macro F1 score to early stopping in training__. This is done to align with the scoring strategy.\n",
    "- Categoricals are turned into numbers with proper mapping instead of blind label encoding.\n",
    "- __OHE if reversed into label encoding, as it is easier to digest for a tree model.__ This trick would be harmful for non-tree models, so be careful.\n",
    "- __idhogar is NOT used in training.__ The only way it could have any info would be if there is a data leak. We are fighting with poverty here- exploiting leaks will not reduce poverty in any way :)\n",
    "- __There are aggregations done within households and new features are hand-crafted.__ Note, that there are not so many features that can be aggregated, as most are already quoted on household level.\n",
    "- __A voting classifier is used to average over several LightGBM models__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepL 번역\n",
    "__에릭 앙투안 스쿠치마라에 의해 수정됨__ - 이 포크는 미샤 로스비가 작성한 https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro 의 포크이며, 몇 가지 변경 사항이 있습니다:\n",
    "- LightGBM 모델이 XGBoost로 대체되었으며 그에 따라 코드가 업데이트되었습니다.\n",
    "- 또한 랜덤포레스트의 투표 분류기를 장착하고, XGB의 결과를 RF와 조합하고 있습니다.\n",
    "- 몇 가지 추가 기능이 추가되었습니다.\n",
    "- 이전에 삭제되었던 일부 기능은 그대로 유지되었습니다.\n",
    "- 일부 코드가 재구성되었습니다.\n",
    "- 데이터를 한 번 분할하여 LGBM 조기 정지를 위한 검증 데이터로 사용하는 대신 전체 훈련 세트를 훈련할 수 있도록 훈련 중에 데이터를 분할했습니다. 이 경우 K-배 분할보다 이 방법이 더 효과적이라는 것을 알게 되었습니다.\n",
    "\n",
    "몇 가지 추가 기능은 Kuriyaman의 https://www.kaggle.com/kuriyaman1002/reduce-features-140-84-keeping-f1-score 에서 가져왔습니다.\n",
    "\n",
    "__원본 커널의 참고 사항(EAS에서 편집)__:\n",
    "이 커널은 https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro 을 거의 따르지만 하이퍼파라미터 최적화를 실행하는 대신 해당 커널의 최적 값을 사용하므로 더 빠르게 실행됩니다.\n",
    "\n",
    "몇 가지 핵심 사항:\n",
    " \n",
    "- __이 커널은 세대주만 대상으로 훈련을 실행합니다__ (세대별 집계 추출 후). 이는 발표된 채점 시작 시점을 따릅니다: 채점에는 세대주만 사용된다는 점에 유의하세요. 모든 가구원은 시험 + 샘플 제출에 포함되지만, 가구주만 채점됩니다. (데이터 설명에서). 그러나 현재로서는 세대주가 아닌 가구원도 평가에 포함되는 것으로 보입니다(https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115 참조). 실제로 완전 예측은 약 0.4 PLB 점수를 주는 반면, 비가구원 항목을 모두 1등급으로 대체하면 약 0.2 PLB 점수로 떨어집니다.\n",
    "\n",
    "- __클래스 주파수의 균형을 맞추는 것이 매우 중요한 것 같습니다__. 훈련된 모델을 밸런싱하지 않으면 ~0.39 PLB / ~0.43 로컬 테스트가 나오는 반면 밸런싱 리드를 추가하면 ~0.42 PLB / 0.47 로컬 테스트가 나옵니다. 수작업으로 할 수도 있고 언더샘플링을 통해 달성할 수도 있습니다. 하지만 가장 간단하고 언더샘플링에 비해 더 강력한 방법은 sklearn API의 LightGBM 모델 생성자에서 class_weight='balanced'를 설정하는 것입니다.\n",
    "\n",
    "- __이 커널은 매크로 F1 점수를 사용하여 훈련에서 조기 중단합니다__. 이는 채점 전략에 맞추기 위해 수행됩니다.\n",
    "- 범주형은 블라인드 레이블 인코딩 대신 적절한 매핑을 통해 숫자로 변환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  #linear algebra\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "# from sklearn.externals.joblib import Parallel, delayed    # 업데이트 후 미지원\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following categorical mapping originates from [this kernel](https://www.kaggle.com/code/mlisovyi/categorical-variables-encoding-function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# this only transforms the idhogar field, the other things this function used to do are done elsewhere\n",
    "\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "\n",
    "# plot feature importance for sklearn decision trees\n",
    "def feature_importance(forest, X_train, display_results=True):\n",
    "    ranked_list = []\n",
    "    zero_features = []\n",
    "\n",
    "    importances = forest.feature_importances_\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    if display_results:\n",
    "        # Print the feature ranking\n",
    "        print(\"Feature ranking:\")\n",
    "    \n",
    "    for f in range(X_train.shape[1]):\n",
    "        if display_results:\n",
    "            print(\"%d. feature %d (%f)\" % (f +1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n",
    "        \n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "\n",
    "        if importances[indices[f]] == 0.0:\n",
    "            zero_features.append(X_train.columns[indices[f]])\n",
    "    \n",
    "    return ranked_list, zero_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also feature engineering magic happening here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'),\n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man)fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('humna_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2')\n",
    "                 ]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhod', 'tamviv'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "    \n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)\n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "    \n",
    "    # aggregation rules over household\n",
    "    aggs_num = {'age': ['min', 'max', 'mean'],\n",
    "                'escolar': ['min', 'max', 'mean']\n",
    "                }\n",
    "    \n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean', 'count']\n",
    "    \n",
    "    # aggregation over household\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + '_' + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "    \n",
    "    # Drop id's\n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert one hot encoded fields to label encoding\n",
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu',\n",
    "               'epared', 'etecho', 'eviv', 'estadocivi;', 'parentesco',\n",
    "               'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'moelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        # deal with those OHE, where there is a sum over columns == 0\n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'.format(s_))\n",
    "            # dummy column name to be added\n",
    "            col_dummy = s_ + '_dummy'\n",
    "            # add the column to the dataframe\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            # add the name to the list of columns to be label-encoded\n",
    "            cols_s_.append(col_dummy)\n",
    "            # proof-check, that now the category is complete\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                print(\"The category completion did not work\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./input/004_costa-rican-household-poverty-prediction/train.csv')\n",
    "test = pd.read_csv('./input/004_costa-rican-household-poverty-prediction/test.csv')\n",
    "\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    # encode the idhogar\n",
    "    encode_data(df_)\n",
    "\n",
    "    # creaete aggregate features\n",
    "    return do_features(df_)\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up some missing data and convert objects to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some dependencies are Na, fill those with the squeare root of the square\n",
    "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/skooch/xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
