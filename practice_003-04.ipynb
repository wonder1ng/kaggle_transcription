{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[이유한님] 캐글 코리아 캐글 스터디 커널 커리큘럼](https://kaggle-kr.tistory.com/32)\n",
    "\n",
    "[3rd level. Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk)\n",
    "\n",
    "[LightGBM 7th place solution](https://www.kaggle.com/code/jsaguiar/lightgbm-7th-place-solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "KAGGLE HOME CREDIT DEFAULT RISK COMPETITION\n",
    "Adapted from one of the models used in 7th place solution ensemble.\n",
    "For more details about our solution please check this discussion:\n",
    "https://www.kaggle.com/competitions/home-credit-default-risk/discussion/64580\n",
    "Another similar version is also available at Github:\n",
    "Http://github.com/js-aguiar/home-credit-default-competition\n",
    "\n",
    "This model uses LightGBM with goss and label encode for the application's\n",
    "categorical features. Other tables are using one-hot encode with mean,\n",
    "sum and a few different functions to aggregate. The main ideia was to add\n",
    "more time related features like last application and last X months aggregations.\n",
    "There are also aggregations for specific loan types and status as well as\n",
    "ratios between tables. Configurations are in line 785\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from scipy.stats import kurtosis, iqr, skew\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def main(debug=False):\n",
    "    num_rows = 30000 if debug else None\n",
    "    with timer(\"application_train and application_test\"):\n",
    "        df = get_train_test(DATA_DIRECTORY, num_rows=num_rows)\n",
    "        print(\"Application dataframe shape: \", df.shape)\n",
    "    with timer(\"Bureau and bureau_balance data\"):\n",
    "        bureau_df = get_bureau(DATA_DIRECTORY, num_rows=num_rows)\n",
    "        df = pd.merge(df, bureau_df, on='SK_ID_CURR', how='left')\n",
    "        print(\"Bureau dataframe shape: \", bureau_df.shape)\n",
    "        del bureau_df; gc.collect()\n",
    "    with timer(\"previous_application\"):\n",
    "        prev_df = get_previous_applications(DATA_DIRECTORY, num_rows)\n",
    "        df = pd.merge(df, prev_df, on='SK_ID_CURR', how='left')\n",
    "        print('Previous dataframe shape:', prev_df.shape)\n",
    "        del prev_df; gc.collect()\n",
    "\n",
    "\n",
    "# ------------------------- LIGHTGBM MODEL -------------------------\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------- APPLICATION PIPELINE -------------------------\n",
    "def get_train_test(path, num_rows = None):\n",
    "    \"\"\"Process application_train.csv and application_test.csv and return a pandas dataframe. \"\"\"\n",
    "    train = pd.read_csv(os.path.join(path, 'application_train.csv'), nrows=num_rows)\n",
    "    test = pd.read_csv(os.path.join(path, 'application_train.csv'), nrows=num_rows)\n",
    "    df = train.append(test)\n",
    "    del train, test; gc.collect()\n",
    "    # Data cleaning\n",
    "    df = df[df['CODE_GENDER'] != 'XNA'] # 4 people with XNA code gender\n",
    "    df = df[df['AMT_INCOME_TOTAL'] < 20000000]  # Max income in test is 4M; train has a 117M value\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # Flag_document features - count and kurtosis\n",
    "    docs = [f for f in df.columns if 'FLAG_DOC' in f]\n",
    "    df['DOCUMENT_COUNT'] = df[docs].sum(axis=1)\n",
    "    df['NEW_DOC_KURT'] = df[docs].kurtosis(axis=1)\n",
    "    # Categorical age - based on target=1 plot\n",
    "    df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n",
    "\n",
    "    # New features based on External sources\n",
    "    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
    "    df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n",
    "    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
    "        feature_name = 'EXT_SOURCE_{}'.format(function_name.upper())\n",
    "        df[feature_name] = eval('np.{}'.format(function_name))(\n",
    "            df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "        \n",
    "    # Credit ratios\n",
    "    df['CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "    df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    # Income ratios\n",
    "    df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "    df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "    df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n",
    "    # Time ratios\n",
    "    df['EMPLOYED_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] / df['DAYS_BIRTH']\n",
    "    df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "    df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "    df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "\n",
    "    # Groupby: Statistics for applications in the same group\n",
    "    group = ['ORGANIZATION_TYPE', 'NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE', 'AGE_RANGE', 'CODE_GENDER']\n",
    "    df = do_median(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_MEDIAN')\n",
    "    df = do_std(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_STD')\n",
    "    df = do_mean(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_MEAN')\n",
    "    df = do_std(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_STD')\n",
    "    df = do_mean(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_MEAN')\n",
    "    df = do_std(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_STD')\n",
    "    df = do_mean(df, group, 'AMT_CREDIT', 'GROUP_CREDIT_MEAN')\n",
    "    df = do_mean(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_MEAN')\n",
    "    df = do_std(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_STD')\n",
    "    \n",
    "    # Encode ategorical features (LabelEncoder)\n",
    "    df, le_encoded_cols = label_encoder(df, None)\n",
    "    df = drop_application_columns(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_application_columns(df):\n",
    "    \"\"\"Drop features based on permutation feature importance.\"\"\"\n",
    "    drop_list = [\n",
    "    'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'HOUR_APPR_PROCESS_START',\n",
    "    'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'FLAG_PHONE',\n",
    "    'FLAG_OWN_REALTY', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n",
    "    'REG_CITY_NOT_WORK_CITY', 'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "    'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_YEAR', \n",
    "    'COMMONAREA_MODE', 'NONLIVINGAREA_MODE', 'ELEVATORS_MODE', 'NONLIVINGAREA_AVG',\n",
    "    'FLOORSMIN_MEDI', 'LANDAREA_MODE', 'NONLIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MODE',\n",
    "    'FLOORSMIN_AVG', 'LANDAREA_AVG', 'FLOORSMIN_MODE', 'LANDAREA_MEDI',\n",
    "    'COMMONAREA_MEDI', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'BASEMENTAREA_AVG',\n",
    "    'BASEMENTAREA_MODE', 'NONLIVINGAPARTMENTS_MEDI', 'BASEMENTAREA_MEDI', \n",
    "    'LIVINGAPARTMENTS_AVG', 'ELEVATORS_AVG', 'YEARS_BUILD_MEDI', 'ENTRANCES_MODE',\n",
    "    'NONLIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'LIVINGAPARTMENTS_MEDI',\n",
    "    'YEARS_BUILD_MODE', 'YEARS_BEGINEXPLUATATION_AVG', 'ELEVATORS_MEDI', 'LIVINGAREA_MEDI',\n",
    "    'YEARS_BEGINEXPLUATATION_MODE', 'NONLIVINGAPARTMENTS_AVG', 'HOUSETYPE_MODE',\n",
    "    'FONDKAPREMONT_MODE', 'EMERGENCYSTATE_MODE'\n",
    "    ]\n",
    "    # Drop most flag document columns\n",
    "    for doc_num in [2,4,5,6,7,8,10,11,12,13,14,15,16,17,19,20,21]:\n",
    "        drop_list.append('FLAG_DOCUMENT_{}'.format(doc_num))\n",
    "    df.drop(drop_list, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_age_label(days_birth):\n",
    "    \"\"\"Return the age group label (int). \"\"\"\n",
    "    age_years = -days_birth / 365\n",
    "    if age_years < 27: return 1\n",
    "    elif age_years < 40: return 2\n",
    "    elif age_years < 50: return 3\n",
    "    elif age_years < 65: return 4\n",
    "    elif age_years < 99: return 5\n",
    "    else: return 0\n",
    "        \n",
    "\n",
    "\n",
    "# ------------------------- BUREAU PIPELINE -------------------------\n",
    "\n",
    "def get_bureau(path, num_rows=None):\n",
    "    \"\"\"Process bureau.csv and bureau_balance.csv and return a pandas dataframe.\"\"\"\n",
    "    bureau = pd.read_csv(os.path.join(path, 'bureau.csv'), nrows=num_rows)\n",
    "    # Credit duration and credit/account end date difference\n",
    "    bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n",
    "    bureau['ENDDATE_DIF'] = -bureau['DAYS_CREDIT_ENDDATE'] + bureau['DAYS_ENDDATE_FACT']\n",
    "    # Credit o debt ratio and difference\n",
    "    bureau['DEBT_PERCENTAGE'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUN'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['CREDIT_TO_ANNUITY_RATIO'] = bureau['AMT_CREDIT_SUN'] - bureau['AMT_ANNUITY']\n",
    "\n",
    "    # One-hot encoder\n",
    "    bureau, categoricla_cols = one_hot_encoder(bureau, nan_as_category=False)\n",
    "    # Join burea balance features\n",
    "    bureau = bureau.merge(get_bureau_balance(path, num_rows), how='left', on='KS_ID_BUREAU')\n",
    "    # Flag months with late payments (days past due)\n",
    "    bureau['STATUS_12345'] = 0\n",
    "    for i in range(1, 6):\n",
    "        bureau['STATUS_12345'] += bureau['STATUS_{}'.format(i)]\n",
    "\n",
    "    # Aggregate by number of months in balance and merge with bureau (loan length agg)\n",
    "    features = ['AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_CREDIT_SUN',\n",
    "                'AMT_CREDIT_SUM_DEBT', 'DEBT_PERCENTAGE', 'DEBT_CREDIT_DIFF', 'STATUS_0', 'STATUS_12345']\n",
    "    agg_length = bureau.groupby('MONTHS_BALANCE_SIZE')[features].mean().reset_index()\n",
    "    agg_length.rename({feat: 'LL_' + feat for feat in features}, axis=1, inplace=True)\n",
    "    bureau = bureau.merge(agg_length, how='left', on='MONTH_BALANCE_SIZE')\n",
    "    del agg_length; gc.collect()\n",
    "\n",
    "    def get_bureau_balance(path, num_rows=None):\n",
    "        bb = pd.read_csv(os.path.join(path, 'bureau_balance.csv'), nrows=num_rows)\n",
    "        bb, categoricla_cols = one_hot_encoder(bb, nan_as_category=False)\n",
    "        # Calculate rate for each category with decay\n",
    "        bb_processed = bb.groupby('SK_ID_BUREAU')[categoricla_cols].mean().reset_index()\n",
    "        # Min, Max, Count and mean duration of payments (months)\n",
    "        agg = {'MONTHS_BALANCE': ['min', 'max', 'mean', 'size']}\n",
    "        bb_processed = group_and_merge(bb, bb_processed, '', agg, 'SK_ID_BUREAU')\n",
    "        del bb; gc.collect()\n",
    "        return bb_processed\n",
    "    \n",
    "# ------------------------- PREVIOUS PIPELINE -------------------------\n",
    "\n",
    "def get_previous_applications(path, num_rows=None):\n",
    "    \"\"\"Process previous_application.csv and return a pandas dataframe.\"\"\"\n",
    "    prev = pd.readcsv(os.path.join(path, 'previus_application.csv'), nrows=num_rows)\n",
    "    pay = pd.readcsv(os.path.join(path, 'installments_payments.csv'), nrows=num_rows)\n",
    "\n",
    "    # One-hot encode most important categorical features\n",
    "    ohe_columns = [\n",
    "        'NAME_CONTRACT_STATUS', 'NAME_CONTRACT_TYPE', 'CHANNEL_TYPE',\n",
    "        'NAME_TYPE_SUITE', 'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION',\n",
    "        'NAME_PRODUCT_TYPE', 'NAME_CLIENT_TYPE']\n",
    "    prev, categorical_cols = one_hot_encoder(prev, ohe_columns, nan_as_category=False)\n",
    "    \n",
    "    # Feature engineering: ratios and difference\n",
    "    prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "    prev['APPLICATION_CREDIT_RATIO'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT'] / prev['AMT_ANNUITY']\n",
    "    prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] / prev['AMT_CREDIT']\n",
    "    # insert ratio on previous application (simplified)\n",
    "    total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
    "    prev['SIMPLE_INTERESTS'] = (total_payment / prev['AMT_CREDIT'] - 1) / prev['CNT_PAYMENT']\n",
    "\n",
    "    # Action loans - approved and not compile yet (last_due 365243)\n",
    "    approved = prev[prev['NAME_CONTRACT_STAUS_Approved'] == 1]\n",
    "    active_df = approved[approved['DAYS_LAST_DUE'] == 365243]\n",
    "    # Find how much was already payed in activate loans (using installments csv)\n",
    "    active_pay = pay[pay['SK_ID_PREV'.isin(active_df['SK_ID_PREV'])]]\n",
    "    active_pay_agg = active_pay.groupby('SK_ID_PRWV')[['AMT_INSTALMENT', 'AMT_PAYMENT']].sum()\n",
    "    active_pay_agg.reset_index(inplace=True)\n",
    "    # Active loans: difference of what was payed and installments\n",
    "    active_pay_agg['INSTALMENT_PAYMENT_DIFF'] = active_pay_agg['AMT_INSTALMENT'] - active_pay_agg['AMT_PAYMENT']\n",
    "    # Merge with active_df\n",
    "    active_df = active_df.merge(active_pay_agg, on='SK_ID_PREV', how='left')\n",
    "    active_df['REMAINING_DEBT'] = active_df['AMT_CREDIT'] - active_df['AMT_PAMENT']\n",
    "    active_df['REMAINING_RATIO'] = active_df['AMT_PAMENT'] / active_df['AMT_CREDIT']\n",
    "    # Perform aggregations for active applications\n",
    "    active_agg_df = group(active_df, 'PREV_ACTIVE_', PREVIOUS_ACTIVE_AGG)\n",
    "    active_agg_df['TOTAL_REPAYMENT_RATIO'] = active_agg_df['PREVIOUS_ACTIVE_AMT_PAYMENT_SUM'] / active_agg_df['PREVIOUS_ACTIVE_AMT_CREDIT_SUM']\n",
    "    del active_pay, active_pay_agg, active_df\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# ------------------------- UTILITY FUNCTIONS -------------------------\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print('{} - done in {:.0f}s'.format(name, time.time() - t0))\n",
    "\n",
    "def group(df_to_agg, prefix, aggregations, aggregate_by='SK_ID_CURR'):\n",
    "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
    "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
    "                               for e in agg_df.columns.tolist()])\n",
    "    return agg_df.reset_index()\n",
    "\n",
    "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by=\"SK_ID_CURR\"):\n",
    "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by=aggregate_by)\n",
    "    return df_to_merge.merge(agg_df, how='left', on=aggregate_by)\n",
    "\n",
    "def do_mean(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].mean().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def do_median(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].median().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def do_std(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].std().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def do_sum(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def one_hot_encoder(df, categorical_columns=None, nan_as_category=True):\n",
    "    \"\"\"Create a new column for each categorical value in categorical columns.\"\"\"\n",
    "    original_columns = list(df.columns)\n",
    "    if not categorical_columns:\n",
    "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    categorical_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, categorical_columns\n",
    "\n",
    "def label_encoder(df, categorical_columns=None):\n",
    "    \"\"\"Encode categorical values as integers (0,1,2,3,...) with pandas.factorizer.\"\"\"\n",
    "    if not categorical_columns:\n",
    "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    for col in categorical_columns:\n",
    "        df[col], unques = pd.factorize(df[col])\n",
    "    return df, categorical_columns\n",
    "\n",
    "# ------------------------- CONFIGURATIONS -------------------------\n",
    "\n",
    "# GENERAL CONFIGURATIONS\n",
    "NUM_THREADS = 4\n",
    "# DATA_DIRECTORY = '../input/'\n",
    "DATA_DIRECTORY = './input/003_home-credit-default-risk/'\n",
    "SUBIMSSION_SUFIX = '_model2_04'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
