{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[이유한님] 캐글 코리아 캐글 스터디 커널 커리큘럼](https://kaggle-kr.tistory.com/32)\n",
    "\n",
    "[Statoil/C-CORE Iceberg Classifier Challenge](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge)\n",
    "\n",
    "[Transfer Learning with VGG-16 CNN+AUG LB 0.1712](https://www.kaggle.com/code/devm2024/transfer-learning-with-vgg-16-cnn-aug-lb-0-1712)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 enviroment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directiory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))    # 윈도우 사용으로 주석처리\n",
    "\n",
    "# Any results you write to the current directory are saved as output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TL;DR\n",
    "**Runs on GPU** There is some compatiblity issue with CPUs\n",
    "\n",
    "1. Hyperparameters in Deep learning are many, tuning them will take weeks or months. Generally researchers do this tuning and publish paper when they find a nice set of architecture which performs better than other.\n",
    "\n",
    "2. Since the model is pre-trained, it converges very fast and you but still you need GPU to use this. Due to some library issues, it doesn't work on CPU.\n",
    "\n",
    "3. For our purpose, we can use those architectures, which are made available by those researchers to us.\n",
    "\n",
    "4. Using those pretrained nets, layers of which already 'knows' how to extract features, we can don't have to tune the hyperparameters. Since they are already trained of some dataset(say imagenet), their pre-trained weights provide a good initialization of weights and because of this, our Convnet converges very fast which otherwise can take days on these deep architectures. That's the idea behind **Transfer Learning**. Examples of which are VGG16, InceptionNet, goolenet, Resnet etc.\n",
    "\n",
    "5. In this kernel we will use pretrained VGG-16 network which performs very well on small size images.\n",
    "\n",
    "**VGG architecture has proved to worked well on small sized images(CIFAR-10)** I expected it to work well for this dataset as well.\n",
    "\n",
    "1. The code also includes the data augmentation steps, thus considerably improving the performance.\n",
    "\n",
    "2. **GPU is needed**\n",
    "\n",
    "Here is the link of the research paper if you are interested. https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "Also here is the doc for keras library: https://keras.io/applications/#vgg16\n",
    "\n",
    "**DeepL 번역**  \n",
    "TL;DR\n",
    "**GPU에서 실행** CPU와 일부 호환성 문제가 있습니다.\n",
    "\n",
    "1. 딥 러닝에는 하이퍼파라미터가 많기 때문에 이를 튜닝하는 데 몇 주 또는 몇 달이 걸립니다. 일반적으로 연구자들은 이러한 튜닝을 통해 다른 아키텍처보다 더 나은 성능을 발휘하는 멋진 아키텍처를 발견하면 논문을 발표합니다.\n",
    "\n",
    "2. 모델이 사전 학습되어 있기 때문에 매우 빠르게 수렴하지만 이를 사용하려면 여전히 GPU가 필요합니다. 일부 라이브러리 문제로 인해 CPU에서는 작동하지 않습니다.\n",
    "\n",
    "3. 연구자들이 우리에게 제공한 아키텍처를 사용할 수 있습니다.\n",
    "\n",
    "4. 이미 특징을 추출하는 방법을 '알고' 있는 사전 훈련된 네트워크를 사용하면 하이퍼파라미터를 조정할 필요가 없습니다. 이미 일부 데이터 세트(예: 이미지넷)에 대해 학습되어 있기 때문에 사전 학습된 가중치는 좋은 가중치 초기화를 제공하며, 이 때문에 이러한 심층 아키텍처에서는 며칠이 걸릴 수 있는 Convnet이 매우 빠르게 수렴합니다. 이것이 바로 **Transfer Learning**의 아이디어입니다. 그 예로 VGG16, InceptionNet, goolenet, Resnet 등이 있습니다.\n",
    "\n",
    "5. 이 커널에서는 작은 크기의 이미지에서 매우 잘 작동하는 사전 학습된 VGG-16 네트워크를 사용할 것입니다.\n",
    "\n",
    "**VGG 아키텍처는 작은 크기의 이미지(CIFAR-10)에서 잘 작동하는 것으로 입증되었습니다** 이 데이터 세트에서도 잘 작동할 것으로 예상했습니다.\n",
    "\n",
    "1. 이 코드에는 데이터 증강 단계도 포함되어 있어 성능이 상당히 향상되었습니다.\n",
    "\n",
    "2. **GPU가 필요합니다**.\n",
    "\n",
    "관심이 있으시다면 연구 논문 링크를 참고하세요. https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "케라스 라이브러리 관련 문서도 여기 있습니다: https://keras.io/applications/#vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mandatory imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "# plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"./input/005_statoil-iceberg-classifier-challenge/train.json\")\n",
    "target_train = train['is_iceberg']\n",
    "test = pd.read_json(\"./input/005_statoil-iceberg-classifier-challenge/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provide the implementation of pretrained VGG, it in it's library so we don't have to build the net by ourselves. Here we are removing the last layer of VGG and putting our sigmoid layer for binary predictions.\n",
    "\n",
    "The following code will NOT WORK, since on kaggle notebook, the weights of model cannot be downloaded, however, you can copy paste the code in your own notebook to make it work.\n",
    "\n",
    "**DeepL 번역**  \n",
    "\n",
    "Keras는 라이브러리에서 사전 학습된 VGG의 구현을 제공하므로 직접 네트워크를 구축할 필요가 없습니다. 여기서는 VGG의 마지막 레이어를 제거하고 이진 예측을 위한 시그모이드 레이어를 넣습니다.\n",
    "\n",
    "다음 코드는 kaggle 노트북에서는 모델의 가중치를 다운로드할 수 없기 때문에 작동하지 않지만, 자신의 노트북에 코드를 복사하여 붙여넣으면 작동할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = train['is_iceberg']\n",
    "test['inc_angle'] = pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle'] = pd.to_numeric(train['inc_angle'], errors='coerce') # We have only 133 NAs.\n",
    "train['inc_angle'] = train['inc_angle'].fillna(method='pad')\n",
    "X_angle = train['inc_angle']\n",
    "test['inc_angle'] = pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "X_test_angle = test['inc_angle']\n",
    "\n",
    "# Generate the training data\n",
    "X_band_1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train['band_1']])\n",
    "X_band_2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train['band_2']])\n",
    "X_band_3 = (X_band_1 + X_band_2) / 2\n",
    "# X_band_3 = np.array([np.full((75, 75), angle).astype(np.float32) for angle in train['inc_angle']])\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis]\n",
    "                          , X_band_2[:, :, :, np.newaxis]\n",
    "                          , X_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "X_band_test_1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test['band_1']])\n",
    "X_band_test_2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test['band_2']])\n",
    "X_band_test_3 = (X_band_test_1 + X_band_test_2) / 2\n",
    "# X_band_test_3 = np.array([np.full((75, 75), angle).astype(np.float32) for angle in test['inc_angle']])\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n",
    "                          , X_band_test_2[:, :, :, np.newaxis]\n",
    "                          , X_band_test_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "# Import Keras.\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.normalization import batch_normalization  # 클래스가 변경된 듯\n",
    "# from keras.layers.merge import Concatenate\n",
    "from keras.layers.merging import Concatenate    # 클래스 변경된 듯\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "# from keras.optimizers import rmsprop  # 클래스 제거된 듯, 데스크탑에서 사용\n",
    "from keras.optimizers import rmsprop_v2   # 랩탑은 환경이 달라 이걸 사용\n",
    "# from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers.activation import LeakyReLU, PReLU    # 호출 경로 변경된 듯\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Data Aug for multi-input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "batch_size = 64\n",
    "# Define the image transformations here\n",
    "gen = ImageDataGenerator(horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         width_shift_range=0.,\n",
    "                         height_shift_range=0.,\n",
    "                         channel_shift_range=0,\n",
    "                         zoom_range=0.2,\n",
    "                         rotation_range=10)\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    genX1 = gen.flow(X1,y, batch_size=batch_size, seed=55)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size, seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            # Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            # np.testing.assert_array_equal(X1i[0], X2i[0])\n",
    "            yield [X1i[0], X2i[1]], X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "def get_callvacks(filepath, patience=2):\n",
    "      es = EarlyStopping('val_loss', patience=10, mode=\"min\")\n",
    "      msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "      return [es, msave]\n",
    "\n",
    "def getVggAngleModel():\n",
    "      input_2 = Input(shape=[1], name=\"angle\")\n",
    "      angle_layer = Dense(1, )(input_2)\n",
    "      base_model = VGG16(weights='imagenet', include_top=False,\n",
    "                         input_shape=X_train.shape[1:])\n",
    "      x = base_model.get_layer('block5_pool').output\n",
    "\n",
    "      x = GlobalMaxPooling2D()(x)\n",
    "      merge_one = concatenate([x, angle_layer])\n",
    "      merge_one = Dense(512, activation='relu', name='fc2')(merge_one)\n",
    "      merge_one = Dropout(0.3)(merge_one)\n",
    "      merge_one = Dense(512, activation='relu', name='fc3')(merge_one)\n",
    "      merge_one = Dropout(0.3)(merge_one)\n",
    "\n",
    "      predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "\n",
    "      # model = Model(input=[base_model.input, input_2], output=predictions)\n",
    "      model = Model(inputs=[base_model.input, input_2], outputs=predictions)  # 오기 수정\n",
    "\n",
    "      # sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "      sgd = SGD(learning_rate=1e-3, decay=1e-6, momentum=0.9, nesterov=True)  # 파라미터 변경\n",
    "      model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=sgd,\n",
    "                    metrics=['accuracy'])\n",
    "      \n",
    "      return model\n",
    "\n",
    "# Using K-fold Cross Validation with Data Augmentation.\n",
    "def myAngleCV(X_train, X_angle, X_test):\n",
    "    K=3\n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=16).split(X_train, target_train))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log = 0\n",
    "    y_valid_pred_log = 0.0 * target_train\n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "      print('\\n===================FOLD=', j)\n",
    "      X_train_cv = X_train[train_idx]\n",
    "      y_train_cv = target_train[train_idx]\n",
    "      X_holdout = X_train[test_idx]\n",
    "      Y_holdout = target_train[test_idx]\n",
    "\n",
    "      # Angle\n",
    "      X_angle_cv = X_angle[train_idx]\n",
    "      X_angle_hold = X_angle[test_idx]\n",
    "\n",
    "      # define file path and get callbacks\n",
    "      file_path = \"%s_aug_model_weights.hdf5\"%j\n",
    "      callbacks = get_callvacks(filepath=file_path, patience=5)\n",
    "      gen_flow = gen_flow_for_two_inputs(X_train_cv, X_angle_cv, y_train_cv)\n",
    "      galaxyModel = getVggAngleModel()\n",
    "      galaxyModel.fit_generator(\n",
    "            gen_flow,\n",
    "            steps_per_epoch=24,\n",
    "            epochs=100,\n",
    "            shuffle=True,\n",
    "            verbose=1,\n",
    "            validation_data=([X_holdout, X_angle_hold], Y_holdout),\n",
    "            callbacks=callbacks)\n",
    "\n",
    "      # Getting the Best Model\n",
    "      galaxyModel.load_weights(filepath=file_path)\n",
    "      # Getting Training Score\n",
    "      score = galaxyModel.evaluate([X_train_cv, X_angle_cv], y_train_cv, verbose=0)\n",
    "      print('Train loss:', score[0])\n",
    "      print('Train accuracy:', score[1])\n",
    "      # Getting Test Sccore\n",
    "      score = galaxyModel.evaluate([X_holdout, X_angle_hold], Y_holdout, verbose=0)\n",
    "      print('Test loss:', score[0])\n",
    "      print('Test accuracy:', score[1])\n",
    "\n",
    "      # Getting validation Score:\n",
    "      pred_valid = galaxyModel.predict([X_holdout, X_angle_hold])\n",
    "      y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "      # Getting Test Scores\n",
    "      temp_test = galaxyModel.predict([X_test, X_test_angle])\n",
    "      y_test_pred_log += temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "      # Getting Train Scores\n",
    "      temp_train = galaxyModel.predict([X_train, X_angle])\n",
    "      y_train_pred_log += temp_train.reshape(temp_train.shape[0])\n",
    "      \n",
    "    y_test_pred_log = y_test_pred_log/K\n",
    "    y_train_pred_log = y_train_pred_log/K\n",
    "\n",
    "    print('\\n Train Log Loss Validation= ', log_loss(target_train, y_train_pred_log))\n",
    "    print('Test Log Loss Validation= ', log_loss(target_train, y_valid_pred_log))\n",
    "    return y_test_pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================FOLD= 0\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 22s 535ms/step - loss: 0.7730 - accuracy: 0.5577 - val_loss: 0.5137 - val_accuracy: 0.7607\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 6s 271ms/step - loss: 0.4856 - accuracy: 0.7594 - val_loss: 0.3137 - val_accuracy: 0.8393\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 6s 260ms/step - loss: 0.3638 - accuracy: 0.8264 - val_loss: 0.2690 - val_accuracy: 0.8692\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 6s 265ms/step - loss: 0.2978 - accuracy: 0.8774 - val_loss: 0.2627 - val_accuracy: 0.8804\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 7s 280ms/step - loss: 0.2863 - accuracy: 0.8778 - val_loss: 0.2675 - val_accuracy: 0.8822\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2689 - accuracy: 0.8919 - val_loss: 0.2484 - val_accuracy: 0.8953\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 7s 310ms/step - loss: 0.2494 - accuracy: 0.8952 - val_loss: 0.2668 - val_accuracy: 0.8785\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 8s 326ms/step - loss: 0.2529 - accuracy: 0.8972 - val_loss: 0.2313 - val_accuracy: 0.9047\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 8s 331ms/step - loss: 0.2198 - accuracy: 0.9071 - val_loss: 0.2228 - val_accuracy: 0.9271\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 8s 322ms/step - loss: 0.2217 - accuracy: 0.9072 - val_loss: 0.2302 - val_accuracy: 0.9178\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 8s 324ms/step - loss: 0.1976 - accuracy: 0.9189 - val_loss: 0.2422 - val_accuracy: 0.9140\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 8s 331ms/step - loss: 0.2142 - accuracy: 0.9090 - val_loss: 0.2259 - val_accuracy: 0.9159\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 7s 299ms/step - loss: 0.1914 - accuracy: 0.9226 - val_loss: 0.2464 - val_accuracy: 0.9103\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 7s 307ms/step - loss: 0.1775 - accuracy: 0.9295 - val_loss: 0.2618 - val_accuracy: 0.9009\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 7s 296ms/step - loss: 0.1940 - accuracy: 0.9226 - val_loss: 0.2265 - val_accuracy: 0.9103\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 7s 296ms/step - loss: 0.1872 - accuracy: 0.9235 - val_loss: 0.2583 - val_accuracy: 0.8879\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 7s 287ms/step - loss: 0.1602 - accuracy: 0.9366 - val_loss: 0.2792 - val_accuracy: 0.8748\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 7s 289ms/step - loss: 0.1491 - accuracy: 0.9440 - val_loss: 0.3034 - val_accuracy: 0.8991\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 7s 289ms/step - loss: 0.1583 - accuracy: 0.9367 - val_loss: 0.2440 - val_accuracy: 0.9178\n",
      "Train loss: 0.1682979315519333\n",
      "Train accuracy: 0.9298409819602966\n",
      "Test loss: 0.22283148765563965\n",
      "Test accuracy: 0.92710280418396\n",
      "17/17 [==============================] - 1s 43ms/step\n",
      "264/264 [==============================] - 13s 50ms/step\n",
      "51/51 [==============================] - 3s 57ms/step\n",
      "\n",
      "===================FOLD= 1\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 8s 308ms/step - loss: 0.7776 - accuracy: 0.5933 - val_loss: 0.4400 - val_accuracy: 0.7832\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 7s 302ms/step - loss: 0.4594 - accuracy: 0.7917 - val_loss: 0.2761 - val_accuracy: 0.8804\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 7s 303ms/step - loss: 0.3654 - accuracy: 0.8231 - val_loss: 0.2584 - val_accuracy: 0.8879\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 7s 300ms/step - loss: 0.3242 - accuracy: 0.8530 - val_loss: 0.2511 - val_accuracy: 0.8879\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 7s 310ms/step - loss: 0.2825 - accuracy: 0.8778 - val_loss: 0.2355 - val_accuracy: 0.9047\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 7s 306ms/step - loss: 0.2699 - accuracy: 0.8754 - val_loss: 0.2497 - val_accuracy: 0.8692\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 7s 301ms/step - loss: 0.2548 - accuracy: 0.8860 - val_loss: 0.2467 - val_accuracy: 0.8766\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 7s 307ms/step - loss: 0.2583 - accuracy: 0.8899 - val_loss: 0.2237 - val_accuracy: 0.8916\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 8s 323ms/step - loss: 0.2301 - accuracy: 0.8985 - val_loss: 0.2199 - val_accuracy: 0.8916\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 7s 298ms/step - loss: 0.2351 - accuracy: 0.8985 - val_loss: 0.2016 - val_accuracy: 0.9140\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 7s 303ms/step - loss: 0.2236 - accuracy: 0.9117 - val_loss: 0.2091 - val_accuracy: 0.8897\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 7s 309ms/step - loss: 0.2049 - accuracy: 0.9103 - val_loss: 0.2126 - val_accuracy: 0.9009\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 7s 313ms/step - loss: 0.1972 - accuracy: 0.9206 - val_loss: 0.2207 - val_accuracy: 0.9009\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 7s 311ms/step - loss: 0.1961 - accuracy: 0.9222 - val_loss: 0.2080 - val_accuracy: 0.9103\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 7s 309ms/step - loss: 0.1893 - accuracy: 0.9252 - val_loss: 0.2018 - val_accuracy: 0.9084\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 7s 312ms/step - loss: 0.1781 - accuracy: 0.9268 - val_loss: 0.2218 - val_accuracy: 0.9065\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 7s 306ms/step - loss: 0.1891 - accuracy: 0.9159 - val_loss: 0.3475 - val_accuracy: 0.8879\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 7s 310ms/step - loss: 0.1932 - accuracy: 0.9229 - val_loss: 0.2049 - val_accuracy: 0.9065\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 7s 311ms/step - loss: 0.1736 - accuracy: 0.9281 - val_loss: 0.2249 - val_accuracy: 0.9009\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 7s 307ms/step - loss: 0.1698 - accuracy: 0.9279 - val_loss: 0.2300 - val_accuracy: 0.8972\n",
      "Train loss: 0.18154269456863403\n",
      "Train accuracy: 0.9251636862754822\n",
      "Test loss: 0.2015579491853714\n",
      "Test accuracy: 0.9140186905860901\n",
      "17/17 [==============================] - 1s 46ms/step\n",
      "264/264 [==============================] - 13s 48ms/step\n",
      "51/51 [==============================] - 2s 48ms/step\n",
      "\n",
      "===================FOLD= 2\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 16s 643ms/step - loss: 0.7049 - accuracy: 0.6252 - val_loss: 0.5403 - val_accuracy: 0.6873\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 8s 328ms/step - loss: 0.4223 - accuracy: 0.8011 - val_loss: 0.3664 - val_accuracy: 0.8202\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 8s 327ms/step - loss: 0.3168 - accuracy: 0.8527 - val_loss: 0.2843 - val_accuracy: 0.8708\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 8s 320ms/step - loss: 0.2680 - accuracy: 0.8827 - val_loss: 0.2715 - val_accuracy: 0.8839\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 8s 322ms/step - loss: 0.2582 - accuracy: 0.8907 - val_loss: 0.2822 - val_accuracy: 0.8839\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 7s 309ms/step - loss: 0.2373 - accuracy: 0.9018 - val_loss: 0.2724 - val_accuracy: 0.8820\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 8s 331ms/step - loss: 0.2388 - accuracy: 0.8926 - val_loss: 0.2697 - val_accuracy: 0.8783\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 8s 331ms/step - loss: 0.2166 - accuracy: 0.9127 - val_loss: 0.2608 - val_accuracy: 0.9064\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 8s 328ms/step - loss: 0.2131 - accuracy: 0.9091 - val_loss: 0.2653 - val_accuracy: 0.8989\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 8s 326ms/step - loss: 0.2001 - accuracy: 0.9093 - val_loss: 0.2662 - val_accuracy: 0.8933\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 8s 330ms/step - loss: 0.1834 - accuracy: 0.9262 - val_loss: 0.2994 - val_accuracy: 0.8764\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 8s 330ms/step - loss: 0.1761 - accuracy: 0.9229 - val_loss: 0.2907 - val_accuracy: 0.8951\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 8s 329ms/step - loss: 0.1859 - accuracy: 0.9247 - val_loss: 0.2892 - val_accuracy: 0.8839\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 8s 339ms/step - loss: 0.1860 - accuracy: 0.9170 - val_loss: 0.2491 - val_accuracy: 0.8989\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 7s 314ms/step - loss: 0.1663 - accuracy: 0.9253 - val_loss: 0.2493 - val_accuracy: 0.9101\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 8s 330ms/step - loss: 0.1195 - accuracy: 0.9506 - val_loss: 0.3219 - val_accuracy: 0.8895\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 8s 326ms/step - loss: 0.1820 - accuracy: 0.9300 - val_loss: 0.2597 - val_accuracy: 0.9007\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 8s 331ms/step - loss: 0.1442 - accuracy: 0.9453 - val_loss: 0.2813 - val_accuracy: 0.8914\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 7s 310ms/step - loss: 0.1401 - accuracy: 0.9440 - val_loss: 0.2736 - val_accuracy: 0.9026\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 8s 327ms/step - loss: 0.1323 - accuracy: 0.9453 - val_loss: 0.3307 - val_accuracy: 0.8989\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 8s 330ms/step - loss: 0.1479 - accuracy: 0.9414 - val_loss: 0.2758 - val_accuracy: 0.8895\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 8s 327ms/step - loss: 0.1315 - accuracy: 0.9500 - val_loss: 0.2877 - val_accuracy: 0.9045\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 8s 330ms/step - loss: 0.1382 - accuracy: 0.9447 - val_loss: 0.2847 - val_accuracy: 0.8933\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 8s 331ms/step - loss: 0.1185 - accuracy: 0.9552 - val_loss: 0.3228 - val_accuracy: 0.8876\n",
      "Train loss: 0.12008969485759735\n",
      "Train accuracy: 0.9542056322097778\n",
      "Test loss: 0.2490740418434143\n",
      "Test accuracy: 0.898876428604126\n",
      "17/17 [==============================] - 1s 44ms/step\n",
      "264/264 [==============================] - 13s 49ms/step\n",
      "51/51 [==============================] - 2s 48ms/step\n",
      "\n",
      " Train Log Loss Validation=  0.16379245952814286\n",
      "Test Log Loss Validation=  0.22447248905817271\n"
     ]
    }
   ],
   "source": [
    "preds = myAngleCV(X_train, X_angle, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission for each day.\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test['id']\n",
    "submission['is_iceberg'] = preds\n",
    "# submission.to_csv('sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
