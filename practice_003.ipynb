{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[이유한님] 캐글 코리아 캐글 스터디 커널 커리큘럼](https://kaggle-kr.tistory.com/32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3rd level. Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk)\n",
    "'#'은 wonder1ng 각주  \n",
    "표기가 없거나 '##~'은 원본 각주(혹은 코드)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Start Here: A Gentle Introduction](https://www.kaggle.com/code/willkoehrsen/start-here-a-gentle-introduction/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Home Credit Default Risk Competition\n",
    "This notebook is intended for those who are new to machine learning competitions or want a gentle introduction to the problem. I purposely avoid jumping into complicated models or joining together lots of data in order to show the basics of how to get started in machine learning! Any comments or suggestions are much appreciated.  \n",
    "  \n",
    "In this notebook, we will take an initial look at the Home Credit default risk machine learning competition currently hosted on Kaggle. The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task:  \n",
    "  \n",
    "- __Supervised__: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features  \n",
    "- __Classification__: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)  \n",
    "  \n",
    "## DeepL 번역\n",
    "이 노트북은 머신 러닝 대회를 처음 접하거나 문제를 가볍게 소개하고자 하는 분들을 위한 것입니다. 머신 러닝을 시작하는 방법의 기본을 보여드리기 위해 복잡한 모델에 뛰어들거나 많은 데이터를 조인하는 것은 의도적으로 피했습니다! 어떤 의견이나 제안도 환영합니다.  \n",
    "  \n",
    "이 노트북에서는 현재 Kaggle에서 주최하는 주택 신용 불이행 위험 머신 러닝 경진대회에 대해 간략히 살펴보겠습니다. 이 대회의 목적은 과거 대출 신청 데이터를 사용하여 신청자가 대출을 상환할 수 있는지 여부를 예측하는 것입니다. 이것은 표준 지도 분류 과제입니다:  \n",
    "  \n",
    "- __감독__: 레이블이 학습 데이터에 포함되어 있으며, 목표는 특징으로부터 레이블을 예측하는 방법을 학습하는 모델을 훈련하는 것입니다.  \n",
    "- __분류__: 레이블은 0(대출금을 제때 상환할 것), 1(대출금을 상환하기 어려울 것)의 이진 변수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The data is provided by Home Credit, a service dedicated to provided lines of credit (loans) to the unbanked population. Predicting whether or not a client will repay a loan or have difficulty is a critical business need, and Home Credit is hosting this competition on Kaggle to see what sort of models the machine learning community can develop to help them in this task.  \n",
    "  \n",
    "There are 7 different sources of data:  \n",
    "  \n",
    "- application_train/application_test: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training application data comes with the TARGET indicating 0: the loan was repaid or 1: the loan was not repaid.\n",
    "- bureau: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n",
    "- bureau_balance: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length.\n",
    "- previous_application: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.\n",
    "- POS_CASH_BALANCE: monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n",
    "- credit_card_balance: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n",
    "- installments_payment: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment.\n",
    "  \n",
    "Moreover, we are provided with the definitions of all the columns (in HomeCredit_columns_description.csv) and an example of the expected submission file.  \n",
    "  \n",
    "In this notebook, we will stick to using only the main application training and testing data. Although if we want to have any hope of seriously competing, we need to use all the data, for now we will stick to one file which should be more manageable. This will let us establish a baseline that we can then improve upon. With these projects, it's best to build up an understanding of the problem a little at a time rather than diving all the way in and getting completely lost!\n",
    "  \n",
    "### DeepL 번역\n",
    "이 데이터는 은행 이용이 어려운 사람들에게 신용 한도(대출)를 제공하는 서비스인 Home Credit에서 제공합니다. 고객이 대출금을 상환할지 또는 어려움을 겪을지 예측하는 것은 중요한 비즈니스 요구 사항이며, Home Credit은 머신 러닝 커뮤니티가 이 작업을 지원하기 위해 어떤 종류의 모델을 개발할 수 있는지 알아보기 위해 Kaggle에서 이 대회를 개최하고 있습니다.  \n",
    "  \n",
    "7가지 데이터 소스가 있습니다:  \n",
    "  \n",
    "- application_train/application_test: Home Credit의 각 대출 신청에 대한 정보가 포함된 기본 훈련 및 테스트 데이터입니다. 모든 대출에는 고유한 행이 있으며 SK_ID_CURR 기능으로 식별됩니다. 훈련 신청 데이터에는 0: 대출이 상환되었거나 1: 대출이 상환되지 않았음을 나타내는 TARGET이 함께 제공됩니다.\n",
    "- 금융기관: 다른 금융기관에서 받은 고객의 이전 크레딧에 관한 데이터입니다. 각 이전 크레딧은 뷰로에서 고유한 행을 가지지만, 신청 데이터의 한 대출에는 여러 개의 이전 크레딧이 있을 수 있습니다.\n",
    "- BUREAU_BALANCE: 뷰로의 이전 크레딧에 대한 월별 데이터입니다. 각 행은 이전 크레딧의 한 달이며, 하나의 이전 크레딧에는 크레딧 기간의 월별로 하나씩 여러 행이 있을 수 있습니다.\n",
    "- 이전_신청: 신청 데이터에 대출이 있는 고객의 홈크레딧에 대한 이전 대출 신청입니다. 신청 데이터의 각 현재 대출에는 여러 개의 이전 대출이 있을 수 있습니다. 각 이전 신청에는 하나의 행이 있으며 SK_ID_PREV 기능으로 식별됩니다.\n",
    "- POS_CASH_BALANCE: 고객이 홈 크레딧을 통해 받은 이전 POS 또는 현금 대출에 대한 월별 데이터입니다. 각 행은 이전 POS 또는 현금 대출의 한 달치이며, 하나의 이전 대출에 여러 행이 있을 수 있습니다.\n",
    "- 신용 카드 잔액: 고객이 홈 크레딧에서 사용한 이전 신용 카드에 대한 월별 데이터입니다. 각 행은 한 달간의 신용카드 잔액이며, 단일 신용카드에 여러 행이 있을 수 있습니다.\n",
    "- installments_payment: 홈 크레딧의 이전 대출에 대한 결제 내역입니다. 결제한 모든 행과 미납한 모든 행에 대해 하나의 행이 있습니다.\n",
    "  \n",
    "또한 모든 열의 정의(HomeCredit_columns_description.csv에 있음)와 예상 제출 파일의 예가 제공됩니다.  \n",
    "  \n",
    "이 노트북에서는 기본 애플리케이션 교육 및 테스트 데이터만 사용하겠습니다. 진지하게 경쟁에 임하려면 모든 데이터를 사용해야 하지만, 지금은 관리하기 쉬운 하나의 파일만 사용하겠습니다. 이를 통해 개선할 수 있는 기준선을 설정할 수 있습니다. 이러한 프로젝트에서는 한꺼번에 뛰어들어 완전히 길을 잃기보다는 한 번에 조금씩 문제에 대한 이해를 쌓는 것이 가장 좋습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, os, warnings, matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir('./input/003_home-credit-default-risk/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train = pd.read_csv('./input/003_home-credit-default-risk/application_train.csv')\n",
    "print('Training data shape:', app_train.shape)\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_test = pd.read_csv('./input/003_home-credit-default-risk/application_test.csv')\n",
    "print('Testing data shape:', app_test.shape)\n",
    "app_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "Exploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.  \n",
    "  \n",
    "### DeepL 번역\n",
    "탐색적 데이터 분석(EDA)은 데이터 내에서 추세, 이상 징후, 패턴 또는 관계를 찾기 위해 통계를 계산하고 수치를 만들어내는 개방형 프로세스입니다. EDA의 목표는 데이터를 통해 무엇을 알 수 있는지 알아내는 것입니다. 일반적으로 높은 수준의 개요로 시작한 다음 데이터에서 흥미로운 영역을 발견하면 특정 영역으로 범위를 좁혀갑니다. 이러한 결과는 그 자체로 흥미로울 수도 있고, 어떤 기능을 사용할지 결정하는 데 도움을 주는 등 모델링 선택에 정보를 제공하는 데 사용될 수도 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine the Distribution of the Target Column\n",
    "## The target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category.\n",
    "## From this information, we see this is an imbalanced class problem. There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can weight the classes by their representation in the data to reflect this imbalance.\n",
    "\n",
    "print(app_train['TARGET'].value_counts())\n",
    "app_train['TARGET'].astype(int).plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine Missing Values\n",
    "## Functuin ti claculate missing values by column# Funct\n",
    "\n",
    "def missing_values_table(df):\n",
    "    ## Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "\n",
    "    ## Percentage of missing values\n",
    "    mis_val_percent = 100*mis_val/len(df)\n",
    "\n",
    "    ## Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "\n",
    "    ## Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(columns={0: 'Missing Values', 1: '% of Total Values'})\n",
    "\n",
    "    ## Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:, 1]!=0].sort_values('% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    ## Print some summary information\n",
    "    print(f'Your selected dataframe has {str(df.shape[1])} columns.\\nThere are {str(mis_val_table_ren_columns.shape[0])} columns that have missing values.')\n",
    "\n",
    "    ## Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Missing values statistics\n",
    "missing_values = missing_values_table(app_train)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can [handle missing values with no need for imputation](https://stats.stackexchange.com/questions/235489/xgboost-can-handle-missing-data-in-the-forecasting-phase). Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now.  \n",
    "  \n",
    "__DeepL 번역__  \n",
    "머신 러닝 모델을 구축할 때 이러한 누락된 값을 채워야 합니다(대입이라고 함). 이후 작업에서는 [대입할 필요 없이 결측값을 처리할 수 있는](https://stats.stackexchange.com/questions/235489/xgboost-can-handle-missing-data-in-the-forecasting-phase) XGBoost와 같은 모델을 사용할 것입니다. 또 다른 옵션은 결측값 비율이 높은 열을 삭제하는 것이지만, 이러한 열이 모델에 도움이 될지 미리 알 수는 없습니다. 따라서 지금은 모든 열을 유지하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Column Types\n",
    "## Number of each type of column\n",
    "app_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of unique classes in each object column\n",
    "app_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables\n",
    "Before we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as LightGBM). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process:  \n",
    "  \n",
    "- Label encoding: assign each unique category in a categorical variable with an integer. No new columns are created.  \n",
    "- One-hot encoding: create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns.  \n",
    "  \n",
    "The problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. In the example above, programmer recieves a 4 and data scientist a 1, but if we did the same process again, the labels could be reversed or completely different. The actual assignment of the integers is arbitrary. Therefore, when we perform label encoding, the model might use the relative value of the feature (for example programmer = 4 and data scientist = 1) to assign weights which is not what we want. If we only have two unique values for a categorical variable (such as Male/Female), then label encoding is fine, but for more than 2 unique categories, one-hot encoding is the safe option.  \n",
    "  \n",
    "There is some debate about the relative merits of these approaches, and some models can deal with label encoded categorical variables with no issues. Here is a good Stack Overflow discussion. I think (and this is just a personal opinion) for categorical variables with many classes, one-hot encoding is the safest approach because it does not impose arbitrary values to categories. The only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by PCA or other dimensionality reduction methods to reduce the number of dimensions (while still trying to preserve information).  \n",
    "  \n",
    "In this notebook, we will use Label Encoding for any categorical variables with only 2 categories and One-Hot Encoding for any categorical variables with more than 2 categories. This process may need to change as we get further into the project, but for now, we will see where this gets us. (We will also not use any dimensionality reduction in this notebook but will explore in future iterations).  \n",
    "\n",
    "### DeepL 번역\n",
    "더 나아가기 전에 성가신 범주형 변수를 처리해야 합니다. 안타깝게도 머신 러닝 모델은 범주형 변수를 처리할 수 없습니다(LightGBM과 같은 일부 모델 제외). 따라서 이러한 변수를 모델에 넘기기 전에 숫자로 인코딩(표현)하는 방법을 찾아야 합니다. 이 프로세스를 수행하는 방법에는 크게 두 가지가 있습니다:  \n",
    "  \n",
    "- 레이블 인코딩: 범주형 변수의 각 고유 카테고리를 정수로 할당합니다. 새 열이 생성되지 않습니다. 아래에 예가 나와 있습니다. 범주형 변수는 다루지 않습니다(LightGBM과 같은 일부 모델 제외). 따라서 이러한 변수를 모델에 넘기기 전에 숫자로 인코딩(표현)하는 방법을 찾아야 합니다. 이 프로세스를 수행하는 방법에는 크게 두 가지가 있습니다:\n",
    "\n",
    "- 레이블 인코딩: 범주형 변수의 각 고유 카테고리를 정수로 할당합니다. 새 열이 생성되지 않습니다.  \n",
    "- 원핫 인코딩: 범주형 변수의 각 고유 범주에 대해 새 열을 만듭니다. 각 관측값은 해당 범주에 해당하는 열에 1을, 다른 모든 새 열에 0을 받습니다.  \n",
    "  \n",
    "라벨 인코딩의 문제점은 카테고리에 임의의 순서를 부여한다는 것입니다. 각 카테고리에 할당된 값은 무작위이며 카테고리의 고유한 측면을 반영하지 않습니다. 위의 예에서 프로그래머는 4를, 데이터 과학자는 1을 받았지만, 동일한 프로세스를 다시 수행하면 레이블이 뒤바뀌거나 완전히 달라질 수 있습니다. 정수의 실제 할당은 임의적입니다. 따라서 레이블 인코딩을 수행할 때 모델은 기능의 상대적 값(예: 프로그래머 = 4, 데이터 과학자 = 1)을 사용하여 우리가 원하는 것과는 다른 가중치를 할당할 수 있습니다. 범주형 변수에 대해 고유한 값이 두 개만 있는 경우(예: 남성/여성) 레이블 인코딩을 사용해도 괜찮지만 고유한 범주가 두 개 이상인 경우에는 원핫 인코딩이 안전한 옵션입니다.  \n",
    "  \n",
    "이러한 접근 방식의 상대적인 장점에 대한 논쟁이 있으며, 일부 모델은 레이블 인코딩된 범주형 변수를 문제 없이 처리할 수 있습니다. 스택 오버플로에 대한 좋은 토론이 여기 있습니다. 많은 클래스가 있는 범주형 변수의 경우 원핫 인코딩이 범주에 임의의 값을 부과하지 않기 때문에 가장 안전한 접근 방식이라고 생각합니다(이것은 개인적인 의견일 뿐입니다). 원핫 인코딩의 유일한 단점은 카테고리가 많은 범주형 변수의 경우 피처(데이터의 차원) 수가 폭발적으로 증가할 수 있다는 것입니다. 이 문제를 해결하기 위해 원핫 인코딩을 수행한 다음 PCA 또는 다른 차원 축소 방법을 사용하여 차원 수를 줄이면서(정보를 보존하려고 노력하면서) 차원 수를 줄일 수 있습니다.  \n",
    "  \n",
    "이 노트북에서는 카테고리가 2개뿐인 범주형 변수에는 라벨 인코딩을 사용하고, 카테고리가 2개 이상인 범주형 변수에는 원핫 인코딩을 사용하겠습니다. 이 프로세스는 프로젝트가 진행됨에 따라 변경해야 할 수도 있지만, 지금은 어떤 결과를 얻을 수 있는지 살펴보겠습니다. (이 노트북에서는 차원 축소도 사용하지 않지만 향후 반복 작업에서 살펴볼 예정입니다).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "## Iterate through the columns\n",
    "for col in app_train:\n",
    "    if app_train[col].dtype=='object':\n",
    "        ## If 2 or fewer unique categories\n",
    "        if len(list(app_train[col].unique()))<=2:\n",
    "            ## Train on the training data\n",
    "            le.fit(app_train[col])\n",
    "            ## Transform both training and testing data\n",
    "            app_train[col] = le.transform(app_train[col])\n",
    "            app_test[col] = le.transform(app_test[col])\n",
    "\n",
    "            ## Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "\n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one-hot encoding of categorical variables\n",
    "app_train = pd.get_dummies(app_train)\n",
    "app_test = pd.get_dummies(app_test)\n",
    "\n",
    "print('Training Features shape:', app_train.shape)\n",
    "print('Testing Features shape:', app_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning Training and Testing Data\n",
    "There need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to align the dataframes. First we extract the target column from the training data (because this is not in the testing data but we need to keep this information). When we do the align, we must make sure to set axis = 1 to align the dataframes based on the columns and not on the rows!  \n",
    "  \n",
    "### Deepl  번역\n",
    "학습 데이터와 테스트 데이터 모두에 동일한 특징(열)이 있어야 합니다. 원핫 인코딩으로 인해 학습 데이터에 더 많은 열이 생성되었는데, 이는 테스트 데이터에 표현되지 않은 범주가 있는 범주형 변수가 일부 있었기 때문입니다. 학습 데이터에서 테스트 데이터에 없는 열을 제거하려면 데이터 프레임을 정렬해야 합니다. 먼저 학습 데이터에서 대상 열을 추출합니다(테스트 데이터에는 없지만 이 정보를 유지해야 하므로). 정렬을 할 때 행이 아닌 열을 기준으로 데이터 프레임을 정렬하려면 축을 1로 설정해야 합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "\n",
    "## Align the training and testing data, kepp only columns present in both dataframes\n",
    "app_train, app_test = app_train.align(app_test, join='inner', axis=1)   # data_1.align(data_2): data 2개를 각 조인하여 (data_1, data_2)의 조인된 튜플로 반환\n",
    "\n",
    "## Add the target back in\n",
    "app_train['TARGET'] = train_labels\n",
    "\n",
    "print('Training Features shape:', app_train.shape)\n",
    "print('Testing Features shape:', app_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Exploratory Data Analysis\n",
    "  \n",
    "__Anomalies__  \n",
    "One problem we always want to be on the lookout for when doing EDA is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies quantitatively is by looking at the statistics of a column using the describe method. The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:  \n",
    "  \n",
    "### DeepL 번역\n",
    "__이상 징후__  \n",
    "EDA를 수행할 때 항상 주의해야 할 문제 중 하나는 데이터 내의 이상 현상입니다. 이는 숫자를 잘못 입력하거나 측정 장비의 오류로 인한 것일 수도 있고, 유효하지만 극단적인 측정값일 수도 있습니다. 이상값을 정량적으로 지원하는 한 가지 방법은 설명 방법을 사용하여 열의 통계를 살펴보는 것입니다. DAYS_BIRTH 열의 숫자는 현재 대출 신청을 기준으로 기록되기 때문에 음수입니다. 이러한 통계를 연도 단위로 보려면 -1을 곱하고 1년의 일수로 나누면 됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(app_train['DAYS_BIRTH']/-365).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train['DAYS_EMPLOYED'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')\n",
    "plt.xlabel('Days Employment');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom = app_train[app_train['DAYS_EMPLOYED']==365243]\n",
    "non_anom = app_train[app_train['DAYS_EMPLOYED']!=365243]\n",
    "print('The non-anomalies default on %0.2f%% of loans' %(100*non_anom['TARGET'].mean()))\n",
    "print('The anomalies default on %0.2f%% of loans' %(100*anom['TARGET'].mean()))\n",
    "print('There are %d anomalous days of employment' %(len(anom)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an anomalous flag column\n",
    "app_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED']==365243\n",
    "## Replace the anomalous values with nan\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n",
    "app_train['DAYS_EMPLOYED'].plot.hist(title='Days Employment Histogram')\n",
    "plt.xlabel('Days Employment');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_test['DAYS_EMPLOYED_ANOM'] = app_test['DAYS_EMPLOYED']==365243\n",
    "app_test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace=True)\n",
    "\n",
    "print('There are %d anomalies in the test data out of %d entries' %(app_test['DAYS_EMPLOYED_ANOM'].sum(), len(app_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "Now that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the .corr dataframe method.  \n",
    "  \n",
    "The correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:  \n",
    "\n",
    "- .00-.19 “very weak”\n",
    "- .20-.39 “weak”\n",
    "- .40-.59 “moderate”\n",
    "- .60-.79 “strong”\n",
    "- .80-1.0 “very strong”\n",
    "  \n",
    "### DeepL 번역\n",
    "범주형 변수와 이상값을 다루었으므로 이제 EDA를 계속해 보겠습니다. 데이터를 이해하는 한 가지 방법은 특징과 대상 간의 상관관계를 찾는 것입니다. .corr 데이터프레임 방법을 사용하여 모든 변수와 대상 간의 피어슨 상관 계수를 계산할 수 있습니다.  \n",
    "  \n",
    "상관 계수는 피처의 '관련성'을 나타내는 가장 좋은 방법은 아니지만, 데이터 내에서 가능한 관계에 대한 아이디어를 제공합니다. 상관 계수의 절대값에 대한 몇 가지 일반적인 해석은 다음과 같습니다:  \n",
    "\n",
    "- .00-.19 \"매우 약함\"\n",
    "- .20-.39 \"약함\"\n",
    "- .40-.59 \"보통\"\n",
    "- .60-.79 \"강함\"\n",
    "- .80-1.0 \"매우 강함\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find correlations with the target and sort\n",
    "correlations = app_train.corr()['TARGET'].sort_values()\n",
    "\n",
    "## Display correlations\n",
    "print('Most Positive Correlations:\\n', correlations.tail(15))\n",
    "print('\\nMost Negative Correlations:\\n', correlations.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Effect of Age on Repayment\n",
    "## Find the correlation of the positive days since birth and target\n",
    "app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\n",
    "app_train['DAYS_BIRTH'].corr(app_train['TARGET'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the style of plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "## plot the distribution of in years\n",
    "plt.hist(app_train['DAYS_BIRTH']/365, edgecolor='k', bins=25)\n",
    "plt.title('Age of Client')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By itself, the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. To visualize the effect of the age on the target, we will next make a [kernel density estimation plot](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) colored by the value of the target. A [kernel density estimate plot shows the distribution of a single variable](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn kdeplot for this graph.  \n",
    "  \n",
    "__DeepL 번역__  \n",
    "  \n",
    "연령 분포는 그 자체로는 모든 연령이 합리적이기 때문에 이상값이 없다는 것 외에는 많은 것을 알려주지 않습니다. 연령이 대상에 미치는 영향을 시각화하기 위해 다음으로 대상의 값에 따라 색을 입힌 [커널 밀도 추정 플롯](https://en.wikipedia.org/wiki/Kernel_density_estimation)(KDE)을 만들겠습니다. [커널 밀도 추정 플롯은 단일 변수의 분포를 보여주며](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) 평활화된 히스토그램으로 생각할 수 있습니다(각 데이터 포인트에서 커널(일반적으로 가우스)을 계산한 다음 모든 개별 커널의 평균을 구하여 하나의 부드러운 곡선을 생성하여 만듭니다). 이 그래프에는 seaborn kdeplot을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "## KDE plot of loans that were repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET']==0, 'DAYS_BIRTH']/365, label='target==0')\n",
    "\n",
    "## KDE plot of loans which were not repaid on time\n",
    "sns.kdeplot(app_train.loc[app_train['TARGET']==1, 'DAYS_BIRTH']/365, label='target==1')\n",
    "\n",
    "## Labeling of plot\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Densitiy')\n",
    "plt.title('Distribution of Ages');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket.  \n",
    "  \n",
    "To make this graph, first we cut the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category.  \n",
    "  \n",
    "__DeepL 번역__  \n",
    "목표 == 1 곡선은 범위의 젊은 쪽 끝으로 치우쳐 있습니다. 이는 유의미한 상관관계는 아니지만(상관계수 -0.07), 이 변수가 목표에 영향을 미치기 때문에 머신 러닝 모델에서 유용하게 사용될 수 있습니다. 이 관계를 다른 방식으로 살펴봅시다. 연령대별 평균 대출 상환 실패율을 살펴보겠습니다.  \n",
    "  \n",
    "이 그래프를 만들기 위해 먼저 연령 범주를 각각 5년 단위로 잘라냅니다. 그런 다음 각 구간차원에 대해 대상의 평균값을 계산하여 각 연령 범주에서 상환하지 않은 대출의 비율을 알려줍니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Age information into a separate dataframe\n",
    "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
    "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH']/365\n",
    "\n",
    "## Bin the age data\n",
    "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num=11))\n",
    "age_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group by yhe bin and calculaye averages\n",
    "age_groups = age_data.groupby('YEARS_BINNED').mean()\n",
    "age_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "## Graph the age bins and the average of the target as a bar plot\n",
    "plt.bar(age_groups.index.astype(str), 100*age_groups['TARGET'])\n",
    "\n",
    "## Plot labeling\n",
    "plt.xticks(rotation=75)\n",
    "plt.xlabel('Age Group (years)')\n",
    "plt.ylabel('Failure to Repay (%)')\n",
    "plt.title('Failure to Repay by Age Group');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exterior Sources\n",
    "The 3 variables with the strongest negative correlations with the target are EXT_SOURCE_1, EXT_SOURCE_2, and EXT_SOURCE_3. According to the documentation, these features represent a \"normalized score from external data source\". I'm not sure what this exactly means, but it may be a cumulative sort of credit rating made using numerous sources of data.  \n",
    "  \n",
    "Let's take a look at these variables.  \n",
    "First, we can show the correlations of the EXT_SOURCE features with the target and with each other.  \n",
    "  \n",
    "### DeepL 번역\n",
    "대상과 가장 강한 음의 상관 관계를 가진 3개의 변수는 EXT_SOURCE_1, EXT_SOURCE_2 및 EXT_SOURCE_3입니다. 문서에 따르면 이러한 기능은 \"외부 데이터 소스의 정규화된 점수\"를 나타냅니다. 이것이 정확히 무엇을 의미하는지 잘 모르겠지만, 다양한 데이터 소스를 사용하여 만든 일종의 누적 신용 등급일 수 있습니다.  \n",
    "  \n",
    "이러한 변수를 살펴보겠습니다.  \n",
    "먼저 EXT_SOURCE 기능의 대상 및 서로 간의 상관 관계를 보여줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the EXT_SOURCE variables and show correlations\n",
    "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "ext_data_corrs = ext_data.corr()\n",
    "ext_data_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "## Heatmap of correlations\n",
    "sns.heatmap(ext_data_corrs, cmap=plt.cm.RdYlBu_r, vmin=-0.25, annot=True, vmax=0.6)\n",
    "plt.title('Correlation Heatmap');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 12))\n",
    "\n",
    "## iterate through the sources\n",
    "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
    "\n",
    "    ## create a new subplot for each source\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    ## plot repaid loans\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET']==0, source], label='target == 0')\n",
    "    ## plot loans that were not repaid\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET']==1, source], label='target == 1')\n",
    "\n",
    "    ## Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % source)\n",
    "    plt.xlabel('%s' % source)\n",
    "    plt.ylabel('Density');\n",
    "plt.tight_layout(h_pad=2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairs Plot\n",
    "As a final exploratory plot, we can make a pairs plot of the EXT_SOURCE variables and the DAYS_BIRTH variable. The Pairs Plot is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle.  \n",
    "  \n",
    "If you don't understand this code, that's all right! Plotting in Python can be overly complex, and for anything beyond the simplest graphs, I usually find an existing implementation and adapt the code (don't repeat yourself)!  \n",
    "  \n",
    "### DeepL 번역\n",
    "마지막 탐색 플롯으로 EXT_SOURCE 변수와 DAYS_BIRTH 변수의 쌍 플롯을 만들 수 있습니다. 쌍 플롯은 단일 변수의 분포뿐만 아니라 여러 쌍의 변수 간의 관계를 볼 수 있기 때문에 훌륭한 탐색 도구입니다. 여기서는 seaborn 시각화 라이브러리와 PairGrid 함수를 사용하여 위쪽 삼각형에는 산점도, 대각선에는 히스토그램, 아래쪽 삼각형에는 2D 커널 밀도 플롯과 상관 계수가 있는 쌍 플롯을 만들고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그래프 상이\n",
    "이상 확인하여 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copy the data for plotting\n",
    "plot_data = ext_data.drop(columns=['DAYS_BIRTH']).copy()\n",
    "\n",
    "## Add in the age of the client in years\n",
    "plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
    "\n",
    "## Drop na values and limit to first 100000 rows\n",
    "plot_data = plot_data.dropna().loc[:100000, :]\n",
    "\n",
    "## Function to calculate correlation coefficient between two columns\n",
    "def corr_func(x, y, **kwargs):\n",
    "    r = np.corrcoef(x,y)[0][1]\n",
    "    ax = plt.gca()\n",
    "    ax.annotate('r = {:.2f}'.format(r), xy=(.2,.8), xycoords=ax.transAxes, size=20)\n",
    "\n",
    "## Create the pairgrid object\n",
    "grid = sns.PairGrid(data=plot_data, height=3, diag_sharey=False,\n",
    "                    # hue='TARGET', vars=[x for x in plot_data.drop(columns='TARGET').columns])\n",
    "                    hue='TARGET', vars=[x for x in list(plot_data.columns) if x != 'TARGET'])\n",
    "## Uper is a scatter plot\n",
    "grid.map_upper(plt.scatter, alpha=0.2)\n",
    "\n",
    "## Diagonal is a histogram\n",
    "grid.map_diag(sns.kdeplot)\n",
    "\n",
    "## Bottom is density plot\n",
    "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r)\n",
    "plt.suptitle('Ext Source and Age Features Pairs Plot', size=32, y=1.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copy the data for plotting\n",
    "# plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n",
    "\n",
    "# # Add in the age of the client in years\n",
    "# plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
    "\n",
    "# # Drop na values and limit to first 100000 rows\n",
    "# plot_data = plot_data.dropna().loc[:100000, :]\n",
    "\n",
    "# # Function to calculate correlation coefficient between two columns\n",
    "# def corr_func(x, y, **kwargs):\n",
    "#     r = np.corrcoef(x, y)[0][1]\n",
    "#     ax = plt.gca()\n",
    "#     ax.annotate(\"r = {:.2f}\".format(r),\n",
    "#                 xy=(.2, .8), xycoords=ax.transAxes,\n",
    "#                 size = 20)\n",
    "\n",
    "# # Create the pairgrid object\n",
    "# grid = sns.PairGrid(data = plot_data, height = 3, diag_sharey=False,\n",
    "#                     hue = 'TARGET', \n",
    "#                     vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n",
    "\n",
    "# # Upper is a scatter plot\n",
    "# grid.map_upper(plt.scatter, alpha = 0.2)\n",
    "\n",
    "# # Diagonal is a histogram\n",
    "# grid.map_diag(sns.kdeplot)\n",
    "\n",
    "# # Bottom is density plot\n",
    "# grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
    "\n",
    "# plt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the EXT_SOURCE_1 and the DAYS_BIRTH (or equivalently YEARS_BIRTH), indicating that this feature may take into account the age of the client.  \n",
    "  \n",
    "__DeepL 번역__  \n",
    "이 플롯에서 빨간색은 상환되지 않은 대출을 나타내고 파란색은 상환된 대출을 나타냅니다. 데이터 내에서 다양한 관계를 확인할 수 있습니다. EXT_SOURCE_1과 DAYS_BIRTH(또는 이에 상응하는 YEARS_BIRTH) 간에 중간 정도의 양의 선형 관계가 있는 것으로 보이며, 이는 이 기능이 고객의 연령을 고려할 수 있음을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Kaggle competitions are won by feature engineering: those win are those who can create the most useful features out of the data. (This is true for the most part as the winning models, at least for structured data, all tend to be variants on gradient boosting). This represents one of the patterns in machine learning: feature engineering has a greater return on investment than model building and hyperparameter tuning. This is a great article on the subject). As Andrew Ng is fond of saying: \"applied machine learning is basically feature engineering.\"  \n",
    "  \n",
    "While choosing the right model and optimal settings are important, the model can only learn from the data it is given. Making sure this data is as relevant to the task as possible is the job of the data scientist (and maybe some [automated tools(주소가 변경되었는데 맞는 동일 도구인지는 모름.)](https://featuretools.alteryx.com/en/stable/) to help us out).  \n",
    "  \n",
    "Feature engineering refers to a geneal process and can involve both feature construction: adding new features from the existing data, and feature selection: choosing only the most important features or other methods of dimensionality reduction. There are many techniques we can use to both create features and select features.  \n",
    "  \n",
    "We will do a lot of feature engineering when we start using the other data sources, but in this notebook we will try only two simple feature construction methods:  \n",
    "  \n",
    " - Polynomial features\n",
    " - Domain knowledge features\n",
    "   \n",
    "## DeepL 번역\n",
    "피처 엔지니어링은 데이터에서 가장 유용한 피처를 만들어내는 사람이 우승하는 대회입니다. (적어도 구조화된 데이터의 경우, 우승 모델은 모두 그라디언트 부스팅에 대한 변형인 경우가 많기 때문에 이는 대부분 사실입니다.) 이는 머신 러닝의 패턴 중 하나인 피처 엔지니어링이 모델 구축 및 하이퍼파라미터 튜닝보다 투자 수익률이 더 높다는 것을 나타냅니다. 이 주제에 대한 훌륭한 기사가 있습니다.) Andrew Ng는 다음과 같이 말합니다: \"응용 머신 러닝은 기본적으로 피처 엔지니어링입니다.\"  \n",
    "  \n",
    "올바른 모델과 최적의 설정을 선택하는 것도 중요하지만, 모델은 주어진 데이터를 통해서만 학습할 수 있습니다. 이 데이터가 작업과 최대한 관련성이 있는지 확인하는 것은 데이터 과학자가 해야 할 일이며, [자동화된 도구(주소가 변경되었는데 맞는 동일 도구인지는 모름.)](https://featuretools.alteryx.com/en/stable/)가 이를 도와줄 수도 있습니다.  \n",
    "  \n",
    "피처 엔지니어링은 유전자 프로세스를 의미하며, 기존 데이터에서 새로운 피처를 추가하는 피처 구성과 가장 중요한 피처만 선택하거나 차원을 줄이는 다른 방법을 선택하는 피처 선택이 모두 포함될 수 있습니다. 피처를 생성하고 피처를 선택하는 데 사용할 수 있는 많은 기술이 있습니다.  \n",
    "  \n",
    "다른 데이터 원본을 사용할 때는 많은 피처 엔지니어링을 수행할 것이지만, 이 노트북에서는 두 가지 간단한 피처 구성 방법만 시도해 보겠습니다:  \n",
    "  \n",
    " - 다항식 피처\n",
    " - 도메인 지식 피처"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features\n",
    "One simple feature construction method is called [polynomial features](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). In this method, we make features that are powers of existing features as well as interaction terms between existing features. For example, we can create variables EXT_SOURCE_1^2 and EXT_SOURCE_2^2 and also variables such as EXT_SOURCE_1 x EXT_SOURCE_2, EXT_SOURCE_1 x EXT_SOURCE_2^2, EXT_SOURCE_1^2 x EXT_SOURCE_2^2, and so on. These features that are a combination of multiple individual variables are called [interaction terms](https://en.wikipedia.org/wiki/Interaction_(statistics)) because they capture the interactions between variables. In other words, while two variables by themselves may not have a strong influence on the target, combining them together into a single interaction variable might show a relationship with the target. [Interaction terms are commonly used in statistical models](https://www.theanalysisfactor.com/interpreting-interactions-in-regression/) to capture the effects of multiple variables, but I do not see them used as often in machine learning. Nonetheless, we can try out a few to see if they might help our model to predict whether or not a client will repay a loan.  \n",
    "  \n",
    "Jake VanderPlas writes about [polynomial features in his excellent book Python for Data Science](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html) for those who want more information.  \n",
    "  \n",
    "In the following code, we create polynomial features using the EXT_SOURCE variables and the DAYS_BIRTH variable. [Scikit-Learn has a useful class called](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) PolynomialFeatures that creates the polynomials and the interaction terms up to a specified degree. We can use a degree of 3 to see the results (when we are creating polynomial features, we want to avoid using too high of a degree, both because the number of features scales exponentially with the degree, and because we can run into [problems with overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)).  \n",
    "\n",
    "### DeepL 번역\n",
    "간단한 피처 구성 방법 중 하나는 [다항식 피처](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)입니다. 이 방법에서는 기존 피처의 거듭제곱과 기존 피처 간의 상호작용 조건인 피처를 만듭니다. 예를 들어 EXT_SOURCE_1^2, EXT_SOURCE_2^2 변수를 만들 수 있고, EXT_SOURCE_1 x EXT_SOURCE_2, EXT_SOURCE_1 x EXT_SOURCE_2^2, EXT_SOURCE_1^2 x EXT_SOURCE_2^2 등의 변수를 만들 수도 있습니다. 여러 개별 변수의 조합인 이러한 특징을 [상호작용항](https://en.wikipedia.org/wiki/Interaction_(statistics))이라고 하는데, 이는 변수 간의 상호작용을 포착하기 때문입니다. 즉, 두 변수는 그 자체로는 대상에 큰 영향을 미치지 않지만, 두 변수를 결합하여 하나의 상호작용 변수로 만들면 대상과의 관계를 나타낼 수 있습니다. 상호작용 용어는 여러 변수의 효과를 포착하기 위해 [통계 모델에서 일반적으로 사용되지만](https://www.theanalysisfactor.com/interpreting-interactions-in-regression/), 머신러닝에서는 자주 사용되지 않는 것으로 알고 있습니다. 그럼에도 불구하고 고객이 대출금을 상환할지 여부를 예측하는 데 모델이 도움이 될 수 있는지 알아보기 위해 몇 가지를 사용해 볼 수 있습니다.  \n",
    "  \n",
    "더 자세한 정보를 원하시는 분들을 위해 Jake VanderPlas는 그의 훌륭한 저서인 [데이터 과학을 위한 파이썬](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html)에서 다항식 함수에 대해 쓰고 있습니다.  \n",
    "  \n",
    "다음 코드에서는 EXT_SOURCE 변수와 DAYS_BIRTH 변수를 사용하여 다항식 특징을 생성합니다. Scikit-Learn에는 지정된 정도까지 다항식과 상호 작용 항을 생성하는 [PolynomialFeatures라는 유용한 클래스](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)가 있습니다. 차수 3을 사용하여 결과를 확인할 수 있습니다(다항식 특징을 생성할 때 차수에 따라 특징의 수가 기하급수적으로 증가하고 [과적합 문제](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)가 발생할 수 있으므로 너무 높은 차수를 사용하지 않는 것이 좋습니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a new dataframe for polynomial features\n",
    "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "\n",
    "## imputer for handling missing values\n",
    "# from sklearn.preprocessing import Imputer 변경된 듯\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "poly_target = poly_features['TARGET']\n",
    "\n",
    "poly_features = poly_features.drop(columns=['TARGET'])\n",
    "\n",
    "## Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "# poly_features_test = imputer.fit_transform(poly_features_test)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "## Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the polynomial features\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "## Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape:', poly_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_transformer.get_feature_names_out(input_features=['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe of the features\n",
    "poly_features = pd.DataFrame(poly_features, columns=poly_transformer.get_feature_names_out(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n",
    "\n",
    "## Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "## Find the correlations with the target\n",
    "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
    "\n",
    "## Display most negative and most positive\n",
    "print(poly_corrs.head(10))\n",
    "print('='*50)\n",
    "print(poly_corrs.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test, columns=poly_transformer.get_feature_names_out(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "## Merge polynomial features into training dataframe\n",
    "poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n",
    "app_train_poly = app_train.merge(poly_features, how='left', on='SK_ID_CURR')\n",
    "\n",
    "## Merge polynomial features into testing dataframe\n",
    "poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n",
    "app_test_poly = app_test.merge(poly_features_test, how='left', on='SK_ID_CURR')\n",
    "\n",
    "## Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join='inner', axis=1)\n",
    "\n",
    "## Print out the new shapes\n",
    "print('Training data with polynomial features shape:', app_train_poly.shape)\n",
    "print('Testing data with polynomial features shape:', app_test_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Knowledge Features\n",
    "Maybe it's not entirely correct to call this \"domain knowledge\" because I'm not a credit expert, but perhaps we could call this \"attempts at applying limited financial knowledge\". In this frame of mind, we can make a couple features that attempt to capture what we think may be important for telling whether a client will default on a loan. Here I'm going to use five features that were inspired by [this script](https://www.kaggle.com/code/jsaguiar/lightgbm-with-simple-features?scriptVersionId=6025993) by Aguiar:  \n",
    "  \n",
    "- __CREDIT_INCOME_PERCENT__: the percentage of the credit amount relative to a client's income\n",
    "- __ANNUITY_INCOME_PERCENT__: the percentage of the loan annuity relative to a client's income\n",
    "- __CREDIT_TERM__: the length of the payment in months (since the annuity is the monthly amount due\n",
    "- __DAYS_EMPLOYED_PERCENT__: the percentage of the days employed relative to the client's age\n",
    "Again, thanks to Aguiar and [his great script](https://www.kaggle.com/code/jsaguiar/lightgbm-with-simple-features?scriptVersionId=6025993) for exploring these features.  \n",
    "  \n",
    "### DeepL 번역\n",
    "저는 신용 전문가가 아니기 때문에 이것을 '도메인 지식'이라고 부르는 것이 옳지 않을 수도 있지만, '제한된 금융 지식을 적용하려는 시도'라고 부를 수 있을 것입니다. 이러한 관점에서 고객의 대출 채무 불이행 여부를 판단하는 데 중요하다고 생각되는 요소를 포착하는 몇 가지 기능을 만들 수 있습니다. 여기서는 [Aguiar의 이 스크립트](https://www.kaggle.com/code/jsaguiar/lightgbm-with-simple-features?scriptVersionId=6025993)에서 영감을 얻은 다섯 가지 기능을 사용하겠습니다:  \n",
    "  \n",
    "- __CREDIT_INCOME_PERCENT__: 고객의 소득 대비 신용 금액의 비율입니다.\n",
    "- __ANNUITY_INCOME_PERCENT__: 고객 소득 대비 대출 연금의 백분율\n",
    "- __CREDIT_TERM__: 월 단위의 지불 기간(연금은 매월 지불해야 하는 금액이므로)\n",
    "- __DAYS_EMPLOYED_PERCENT__: 고객의 나이 대비 고용된 일수의 백분율입니다.\n",
    "  \n",
    "다시 한 번 이러한 기능을 살펴볼 수 있게 해준 [Aguiar와 그의 훌륭한 스크립트](https://www.kaggle.com/code/jsaguiar/lightgbm-with-simple-features?scriptVersionId=6025993)에 감사드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_domain = app_train.copy()\n",
    "app_test_domain = app_test.copy()\n",
    "\n",
    "app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT']/app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY']/app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY']/app_train_domain['AMT_CREDIT']\n",
    "app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED']/app_train_domain['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT']/app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY']/app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY']/app_test_domain['AMT_CREDIT']\n",
    "app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED']/app_test_domain['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize New Variables\n",
    "We should explore these domain knowledge variables visually in a graph. For all of these, we will make the same KDE plot colored by the value of the TARGET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,20))\n",
    "## iterate through the new features\n",
    "for i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n",
    "\n",
    "    ## create a new subplot for each source\n",
    "    plt.subplot(4,1,i+1)\n",
    "    ## plot repaid loans\n",
    "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==0,feature], label='target == 0')\n",
    "    ## plot loans that were not repaid\n",
    "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==1,feature], label='target == 1')\n",
    "\n",
    "    ## Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % feature)\n",
    "    plt.xlabel(str(feature))\n",
    "    plt.ylabel('Density');\n",
    "\n",
    "plt.tight_layout(h_pad=2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "For a naive baseline, we could guess the same value for all examples on the testing set. We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess 0.5 for all observations on the test set. This will get us a Reciever Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition ([random guessing on a classification task will score a 0.5](https://stats.stackexchange.com/questions/266387/can-auc-roc-be-between-0-0-5)).  \n",
    "  \n",
    "Since we already know what score we are going to get, we don't really need to make a naive baseline guess. Let's use a slightly more sophisticated model for our actual baseline: Logistic Regression.  \n",
    "  \n",
    "### Logistic Regression Implementation\n",
    "Here I will focus on implementing the model rather than explaining the details, but for those who want to learn more about the theory of machine learning algorithms, I recommend both An Introduction to Statistical Learning and [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/). Both of these books present the theory and also the code needed to make the models (in R and Python respectively). They both teach with the mindset that the best way to learn is by doing, and they are very effective!  \n",
    "  \n",
    "To get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values (imputation) and normalizing the range of the features (feature scaling). The following code performs both of these preprocessing steps.  \n",
    "  \n",
    "## DeepL 번역\n",
    "순진한 기준선의 경우, 테스트 집합의 모든 예에 대해 동일한 값을 추측할 수 있습니다. 대출을 상환하지 않을 확률을 예측해야 하므로 완전히 확신할 수 없는 경우 테스트 집합의 모든 관측값에 대해 0.5를 추측합니다. 이렇게 하면 경쟁에서 0.5의 곡선 아래 수신자 운영 특성 영역(AUC ROC)을 얻을 수 있습니다([분류 작업에서 무작위로 추측하면 0.5 점수가 나옵니다](https://stats.stackexchange.com/questions/266387/can-auc-roc-be-between-0-0-5)).  \n",
    "  \n",
    "우리는 이미 어떤 점수를 받을지 알고 있기 때문에 순진한 기준선을 추측할 필요가 없습니다. 실제 기준선을 위해 조금 더 정교한 모델을 사용해 보겠습니다: 로지스틱 회귀입니다.  \n",
    "  \n",
    "### 로지스틱 회귀 구현하기\n",
    "여기서는 자세한 설명보다는 모델을 구현하는 데 중점을 두겠지만, 머신 러닝 알고리즘 이론에 대해 더 자세히 알고 싶은 분들에게는 [통계 학습 입문]과 [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)을 추천합니다. 이 두 책 모두 이론과 함께 모델을 만드는 데 필요한 코드(각각 R과 Python)를 제시합니다. 두 책 모두 학습하는 가장 좋은 방법은 실행하는 것이라는 마음가짐으로 가르치고 있으며, 매우 효과적입니다!  \n",
    "  \n",
    "기준선을 얻기 위해 범주형 변수를 인코딩한 후 모든 기능을 사용할 것입니다. 결측값을 채우고(대입) 특징의 범위를 정규화하여(특징 스케일링) 데이터를 전처리할 것입니다. 다음 코드는 이 두 가지 전처리 단계를 모두 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "## Drop the target from the training data\n",
    "if 'TARGET' in app_train:\n",
    "    train = app_train.drop(columns=['TARGET'])\n",
    "else:\n",
    "    train = app_train.copy()\n",
    "\n",
    "## Feature names\n",
    "features = list(train.columns)\n",
    "\n",
    "## Copy of the testing data\n",
    "test = app_test.copy()\n",
    "\n",
    "## Median imputation of missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "## Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "## Fit on the training data\n",
    "imputer.fit(train)\n",
    "\n",
    "## Transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(app_test)\n",
    "\n",
    "## Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "print('Training data sahpe:', train.shape)\n",
    "print('Testing data shape:', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use LogisticRegression [from Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for our first model. The only change we will make from the default model settings is to lower the [regularization parameter](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), C, which controls the amount of overfitting (a lower value should decrease overfitting). This will get us slightly better results than the default LogisticRegression, but it still will set a low bar for any future models.  \n",
    "  \n",
    "Here we use the familiar Scikit-Learn modeling syntax: we first create the model, then we train the model using .fit and then we make predictions on the testing data using .predict_proba (remember that we want probabilities and not a 0 or 1).  \n",
    "  \n",
    "__DeepL__  \n",
    "첫 번째 모델에는 [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)의 로지스틱 회귀를 사용하겠습니다. 기본 모델 설정에서 변경할 유일한 사항은 과적합의 양을 제어하는 [regularization parameter](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)인 C를 낮추는 것입니다(값이 낮을수록 과적합이 감소합니다). 이렇게 하면 기본 LogisticRegression보다 약간 더 나은 결과를 얻을 수 있지만, 향후 모델에 대한 기준은 여전히 낮게 설정됩니다.  \n",
    "  \n",
    "여기에서는 먼저 모델을 생성한 다음 .fit을 사용하여 모델을 학습한 다음 .predict_proba를 사용하여 테스트 데이터에 대한 예측을 수행합니다(0이나 1이 아닌 확률을 원한다는 점을 기억하세요)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## Make the model with the specified regularization parameter\n",
    "log_reg = LogisticRegression(C=0.0001)\n",
    "\n",
    "## Train on the training data\n",
    "log_reg.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make predictions\n",
    "## Make sure to select the second column only\n",
    "log_reg_pred = log_reg.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submission dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = log_reg_pred\n",
    "\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the submisson to a csv file\n",
    "# submit.to_csv('lpg_reg_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Model: Random Forest\n",
    "To try and beat the poor performance of our baseline, we can update the algorithm. Let's try using a Random Forest on the same training data to see how that affects performance. The Random Forest is a much more powerful model especially when we use hundreds of trees. We will use 100 trees in the random forest.  \n",
    "  \n",
    "### DeepL 번역\n",
    "기준선의 낮은 성능을 극복하기 위해 알고리즘을 업데이트할 수 있습니다. 동일한 훈련 데이터에 랜덤 포리스트를 사용하여 성능에 어떤 영향을 미치는지 확인해 보겠습니다. 랜덤 포레스트는 특히 수백 개의 트리를 사용할 때 훨씬 더 강력한 모델입니다. 무작위 포리스트에 100개의 나무를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Make the random forest classifier\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train on the training data\n",
    "random_forest.fit(train, train_labels)\n",
    "\n",
    "## Extract feature importances\n",
    "feature_importance_values = random_forest.feature_importances_\n",
    "feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n",
    "\n",
    "## Make predictions on the test data\n",
    "predictions = random_forest.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a submisson dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = predictions\n",
    "\n",
    "## Save the submission dataframe\n",
    "# submit.to_csv('random_forest_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions using Engineered Features\n",
    "The only way to see if the Polynomial Features and Domain knowledge improved the model is to train a test a model on these features! We can then compare the submission performance to that for the model without these features to gauge the effect of our feature engineering.  \n",
    "  \n",
    "### DeepL 번역\n",
    "다항식 기능과 도메인 지식이 모델을 개선했는지 확인할 수 있는 유일한 방법은 이러한 기능으로 모델을 테스트하는 것입니다! 그런 다음 이러한 기능이 없는 모델의 제출 성능과 비교하여 기능 엔지니어링의 효과를 측정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features_names = list(app_train_poly.columns)\n",
    "\n",
    "## Impute the polynomial features\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "poly_features = imputer.fit_transform(app_train_poly)\n",
    "poly_features_test = imputer.transform(app_test_poly)\n",
    "\n",
    "## Scale the polynomial features\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "poly_features = scaler.fit_transform(poly_features)\n",
    "poly_features_test = scaler.fit_transform(poly_features_test)\n",
    "\n",
    "random_forest_poly = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train on the training data\n",
    "random_forest_poly.fit(poly_features, train_labels)\n",
    "\n",
    "## Make predictions on the test data\n",
    "predictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a submission dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = predictions\n",
    "\n",
    "## Save the submission dataframe\n",
    "# submit.to_csv('random_forest_baseline_engineered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Domain Features\n",
    "Now we can test the domain features we made by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_domain = app_train_domain.drop(columns='TARGET')\n",
    "\n",
    "domain_features_names = list(app_train_domain.columns)\n",
    "\n",
    "## Impute the domainnomial features\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "domain_features = imputer.fit_transform(app_train_domain)\n",
    "domain_features_test = imputer.transform(app_test_domain)\n",
    "\n",
    "## Scale the domainnomial features\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "domain_features = scaler.fit_transform(domain_features)\n",
    "domain_features_test = scaler.transform(domain_features_test)\n",
    "\n",
    "random_forest_domain = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n",
    "\n",
    "## Train on the training data\n",
    "random_forest_domain.fit(domain_features, train_labels)\n",
    "\n",
    "## Extract feature importances\n",
    "feature_importance_values_domain = random_forest_domain.feature_importances_\n",
    "feature_importance_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n",
    "\n",
    "## Make predictions on the test data\n",
    "predictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a submission dataframe\n",
    "submit = app_test[['SK_ID_CURR']]\n",
    "submit['TARGET'] = predictions\n",
    "\n",
    "## Save the submission dataframe\n",
    "# submit.to_csv('random_forest_baseline_domain.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation: Feature Importances\n",
    "As a simple method to see which variables are the most relevant, we can look at the feature importances of the random forest. Given the correlations we saw in the exploratory data analysis, we should expect that the most important features are the EXT_SOURCE and the DAYS_BIRTH. We may use these feature importances as a method of dimensionality reduction in future work.  \n",
    "  \n",
    "### DeepL 번역\n",
    "어떤 변수가 가장 관련성이 높은지 확인하는 간단한 방법으로, 랜덤 포레스트의 특징 중요도를 살펴볼 수 있습니다. 탐색적 데이터 분석에서 확인한 상관관계를 고려할 때, 가장 중요한 특징은 EXT_SOURCE와 DAYS_BIRTH라고 예상할 수 있습니다. 향후 작업에서 이러한 특징 중요도를 차원 축소 방법으로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    '''\n",
    "    Plot importances returened by a model. This can work with any measure of \n",
    "    feature importance provided that higher importance is better.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): feature importances. Must have the features in a column \n",
    "        called 'features' and the importances in a column called 'importance'\n",
    "\n",
    "    Returns:\n",
    "        show a plot of the 15 most importance features\n",
    "\n",
    "        df (dataframe): feature importances sorted by importance (highest to lowest)\n",
    "        with a column for normalized importance\n",
    "    '''\n",
    "\n",
    "    ## Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending=False).reset_index()\n",
    "\n",
    "    ## Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    ## Make a horizontal bar chart of feture importances\n",
    "    plt.figure(figsize=(10,6))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "\n",
    "    ax.barh(list(reversed(list(df.index[:15]))),\n",
    "            df['importance_normalized'].head(15),\n",
    "            align='center', edgecolor='k')\n",
    "    \n",
    "    ## Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "\n",
    "    ## Plot labeling\n",
    "    plt.xlabel('Normalized Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the feature importances for the default features\n",
    "feature_importance_sorted = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the most important features are those dealing with EXT_SOURCE and DAYS_BIRTH. We see that there are only a handful of features with a significant importance to the model, which suggests we may be able to drop many of the features without a decrease in performance (and we may even see an increase in performance.) Feature importances are not the most sophisticated method to interpret a model or perform dimensionality reduction, but they let us start to understand what factors our model takes into account when it makes predictions.  \n",
    "  \n",
    "__DeepL 번역__  \n",
    "예상대로 가장 중요한 기능은 EXT_SOURCE와 DAYS_BIRTH를 처리하는 기능입니다. 모델에서 중요도가 높은 피처가 소수에 불과하다는 것을 알 수 있으며, 이는 성능 저하 없이 많은 피처를 삭제할 수 있음을 시사합니다(심지어 성능이 향상될 수도 있습니다). 피처 중요도는 모델을 해석하거나 차원 축소를 수행하는 가장 정교한 방법은 아니지만, 모델이 예측을 할 때 어떤 요소를 고려하는지 이해할 수 있게 해주므로 향후 작업에서 차원 축소 방법으로 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_domain_sorted = plot_feature_importances(feature_importance_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all four of our hand-engineered features made it into the top 15 most important! This should give us confidence that our domain knowledge was at least partially on track.  \n",
    "  \n",
    "__DeepL 번역__  \n",
    "저희가 직접 엔지니어링한 네 가지 기능이 모두 가장 중요한 상위 15개 기능에 포함된 것을 확인할 수 있었습니다! 이를 통해 우리의 도메인 지식이 적어도 부분적으로는 궤도에 올랐다는 확신을 가질 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "In this notebook, we saw how to get started with a Kaggle machine learning competition. We first made sure to understand the data, our task, and the metric by which our submissions will be judged. Then, we performed a fairly simple EDA to try and identify relationships, trends, or anomalies that may help our modeling. Along the way, we performed necessary preprocessing steps such as encoding categorical variables, imputing missing values, and scaling features to a range. Then, we constructed new features out of the existing data to see if doing so could help our model.  \n",
    "  \n",
    "Once the data exploration, data preparation, and feature engineering was complete, we implemented a baseline model upon which we hope to improve. Then we built a second slightly more complicated model to beat our first score. We also carried out an experiment to determine the effect of adding the engineering variables.  \n",
    "  \n",
    "We followed the general outline of a [machine learning project](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420):  \n",
    "  \n",
    "1. Understand the problem and the data\n",
    "2. Data cleaning and formatting (this was mostly done for us)\n",
    "3. Exploratory Data Analysis\n",
    "4. Baseline model\n",
    "5. Improved model\n",
    "6. Model interpretation (just a little)\n",
    "  \n",
    "Machine learning competitions do differ slightly from typical data science problems in that we are concerned only with achieving the best performance on a single metric and do not care about the interpretation. However, by attempting to understand how our models make decisions, we can try to improve them or examine the mistakes in order to correct the errors. In future notebooks we will look at incorporating more sources of data, building more complex models (by following the code of others), and improving our scores.  \n",
    "  \n",
    "Machine learning competitions do differ slightly from typical data science problems in that we are concerned only with achieving the best performance on a single metric and do not care about the interpretation. However, by attempting to understand how our models make decisions, we can try to improve them or examine the mistakes in order to correct the errors. In future notebooks we will look at incorporating more sources of data, building more complex models (by following the code of others), and improving our scores.  \n",
    "  \n",
    "I hope this notebook was able to get you up and running in this machine learning competition and that you are now ready to go out on your own - with help from the community - and start working on some great problems!  \n",
    "  \n",
    "__Running the notebook__: now that we are at the end of the notebook, you can hit the blue Commit & Run button to execute all the code at once. After the run is complete (this should take about 10 minutes), you can then access the files that were created by going to the versions tab and then the output sub-tab. The submission files can be directly submitted to the competition from this tab or they can be downloaded to a local machine and saved. The final part is to share the share the notebook: go to the settings tab and change the visibility to Public. This allows the entire world to see your work!  \n",
    "  \n",
    "__Follow-up Notebooks__\n",
    "For those looking to keep working on this problem, I have a series of follow-up notebooks:\n",
    "\n",
    "- [Manual Feature Engineering Part One](https://www.kaggle.com/code/willkoehrsen/introduction-to-manual-feature-engineering/notebook)\n",
    "- [Manual Feature Engineering Part Two](https://www.kaggle.com/code/willkoehrsen/introduction-to-manual-feature-engineering-p2/notebook)\n",
    "- [Introduction to Automated Feature Engineering](https://www.kaggle.com/code/willkoehrsen/automated-feature-engineering-basics/notebook)\n",
    "- [Advanced Automated Feature Engineering](https://www.kaggle.com/code/willkoehrsen/tuning-automated-feature-engineering-exploratory/notebook)\n",
    "- [Feature Selection](https://www.kaggle.com/code/willkoehrsen/introduction-to-feature-selection/notebook)\n",
    "- [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/code/willkoehrsen/intro-to-model-tuning-grid-and-random-search/notebook)\n",
    "  \n",
    "As always, I welcome feedback and constructive criticism. I write for Towards Data Science at https://medium.com/@williamkoehrsen/ and can be reached on Twitter at https://twitter.com/koehrsen_will  \n",
    "  \n",
    "## DeepL 번역\n",
    "이 노트북에서는 Kaggle 머신 러닝 경진 대회를 시작하는 방법을 살펴봤습니다. 먼저 데이터, 과제, 제출물을 평가할 메트릭을 이해해야 했습니다. 그런 다음 모델링에 도움이 될 수 있는 관계, 추세 또는 이상 징후를 식별하기 위해 매우 간단한 EDA를 수행했습니다. 그 과정에서 범주형 변수를 인코딩하고, 누락된 값을 대입하고, 피처를 범위로 확장하는 등 필요한 전처리 단계를 수행했습니다. 그런 다음 기존 데이터에서 새로운 기능을 구성하여 모델에 도움이 될 수 있는지 확인했습니다.  \n",
    "  \n",
    "데이터 탐색, 데이터 준비, 기능 엔지니어링이 완료되면 개선하고자 하는 기준 모델을 구현했습니다. 그런 다음 첫 번째 점수를 능가하는 약간 더 복잡한 두 번째 모델을 구축했습니다. 또한 엔지니어링 변수를 추가했을 때 어떤 효과가 있는지 확인하기 위해 실험을 수행했습니다.  \n",
    "  \n",
    "[머신러닝 프로젝트](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420)의 일반적인 개요를 따랐습니다:  \n",
    "  \n",
    "1. 문제와 데이터 이해\n",
    "2. 데이터 정리 및 서식 지정(대부분 저희가 직접 수행)\n",
    "3. 탐색적 데이터 분석\n",
    "4. 기준 모델\n",
    "5. 개선된 모델\n",
    "6. 모델 해석(약간)\n",
    "  \n",
    "머신러닝 경진대회는 단일 메트릭에서 최고의 성능을 달성하는 데만 관심이 있고 해석에는 신경 쓰지 않는다는 점에서 일반적인 데이터 과학 문제와는 약간 다릅니다. 그러나 모델이 어떻게 의사 결정을 내리는지 이해하려고 시도함으로써 모델을 개선하거나 오류를 수정하기 위해 실수를 조사할 수 있습니다. 다음 노트에서는 더 많은 데이터 소스를 통합하고, 다른 사람의 코드를 따라 더 복잡한 모델을 구축하고, 점수를 개선하는 방법을 살펴볼 것입니다.  \n",
    "  \n",
    "기계 학습 대회는 단일 메트릭에서 최고의 성능을 달성하는 데만 관심이 있고 해석에는 신경 쓰지 않는다는 점에서 일반적인 데이터 과학 문제와 약간 다릅니다. 그러나 모델이 어떻게 의사 결정을 내리는지 이해하려고 시도함으로써 모델을 개선하거나 오류를 수정하기 위해 실수를 조사할 수 있습니다. 다음 노트북에서는 더 많은 데이터 소스를 통합하고, 다른 사람의 코드를 따라 더 복잡한 모델을 구축하고, 점수를 개선하는 방법을 살펴볼 것입니다.  \n",
    "  \n",
    "이 노트북이 이번 머신 러닝 경진대회에 참가하는 데 도움이 되었기를 바라며, 이제 커뮤니티의 도움을 받아 혼자서 멋진 문제를 풀어나갈 준비가 되셨기를 바랍니다!  \n",
    "  \n",
    "__Running the notebook__: 이제 노트북의 마지막 단계에 이르렀으므로 파란색 커밋 & 실행 버튼을 눌러 모든 코드를 한 번에 실행할 수 있습니다. 실행이 완료된 후(약 10분 정도 소요) 버전 탭과 출력 하위 탭으로 이동해 생성된 파일에 액세스할 수 있습니다.제출 파일은 이 탭에서 대회에 직접 제출하거나 로컬 컴퓨터에 다운로드하여 저장할 수 있습니다. 마지막 단계는 노트북을 공유하는 것입니다. 설정 탭으로 이동해 공개 여부를 공개로 변경합니다.이렇게 하면 전 세계가 회원님의 작업을 볼 수 있습니다!  \n",
    "  \n",
    "__Follow-up Notebooks__  \n",
    "이 문제를 계속 해결하고 싶은 분들을 위해 후속 노트북 시리즈를 준비했습니다:\n",
    "\n",
    "- [Manual Feature Engineering Part One](https://www.kaggle.com/code/willkoehrsen/introduction-to-manual-feature-engineering/notebook)\n",
    "- [Manual Feature Engineering Part Two](https://www.kaggle.com/code/willkoehrsen/introduction-to-manual-feature-engineering-p2/notebook)\n",
    "- [Introduction to Automated Feature Engineering](https://www.kaggle.com/code/willkoehrsen/automated-feature-engineering-basics/notebook)\n",
    "- [Advanced Automated Feature Engineering](https://www.kaggle.com/code/willkoehrsen/tuning-automated-feature-engineering-exploratory/notebook)\n",
    "- [Feature Selection](https://www.kaggle.com/code/willkoehrsen/introduction-to-feature-selection/notebook)\n",
    "- [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/code/willkoehrsen/intro-to-model-tuning-grid-and-random-search/notebook)\n",
    "  \n",
    "언제나 그렇듯이 저는 피드백과 건설적인 비판을 환영합니다. 저는 'Towards Data Science'에 글을 쓰고 있으며(https://medium.com/@williamkoehrsen/), 트위터(https://twitter.com/koehrsen_will)에서도 연락할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just for Fun: Light Gradient Boosting Machine\n",
    "Now (if you want, this part is entirely optional) we can step off the deep end and use a real machine learning model: the [gradient boosting machine](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/) using the [LightGBM library](https://lightgbm.readthedocs.io/en/latest/Quick-Start.html)! The Gradient Boosting Machine is currently the leading model for learning on structured datasets (especially on Kaggle) and we will probably need some form of this model to do well in the competition. Don't worry, even if this code looks intimidating, it's just a series of small steps that build up to a complete model. I added this code just to show what may be in store for this project, and because it gets us a slightly better score on the leaderboard. In future notebooks we will see how to work with more advanced models (which mostly means adapting existing code to make it work better), feature engineering, and feature selection. See you in the next notebook!  \n",
    "  \n",
    "## DeepL 번역\n",
    "이제 (원한다면 이 부분은 전적으로 선택 사항입니다.) 딥 엔드에서 벗어나 실제 머신 러닝 모델인 [LightGBM 라이브러리]((https://lightgbm.readthedocs.io/en/latest/Quick-Start.html))를 사용하는 [그라디언트 부스팅 머신]((https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/))을 사용할 수 있습니다! 그래디언트 부스팅 머신은 현재 구조화된 데이터 세트(특히 Kaggle에서)에 대한 학습을 위한 선도적인 모델이며, 경쟁에서 좋은 성적을 거두려면 어떤 형태로든 이 모델이 필요할 것입니다. 이 코드가 어렵게 보이더라도 걱정하지 마세요. 완전한 모델을 구축하기 위한 일련의 작은 단계일 뿐입니다. 이 코드를 추가한 이유는 이 프로젝트에 어떤 것이 준비되어 있는지 보여드리기 위해서이며, 순위표에서 조금 더 좋은 점수를 받기 위해서입니다. 다음 노트북에서는 더 고급 모델(대부분 기존 코드를 더 잘 작동하도록 조정하는 것을 의미), 기능 엔지니어링 및 기능 선택으로 작업하는 방법을 살펴볼 것입니다. 다음 노트북에서 뵙겠습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "def model(features, test_features, encoding='ohe', n_folds=5):\n",
    "    \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    ## Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "\n",
    "    ## Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "\n",
    "    ## Remove the ids and target\n",
    "    features = features.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "    ## One Hpt Emcoding\n",
    "    if encoding=='ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "\n",
    "        ## Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join='inner', axis=1)\n",
    "\n",
    "        ## No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    ## Interger label encoding\n",
    "    elif encoding=='le':\n",
    "\n",
    "        ## Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        ## List for storing categorical indices\n",
    "        cat_indices = []\n",
    "\n",
    "        ## Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                ## Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                ## Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    ## Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le\")\n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "\n",
    "    ## Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    ## Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "\n",
    "    ## Create the kfold object\n",
    "    k_fold = KFold(n_splits=n_folds, shuffle=True, random_state=50)\n",
    "\n",
    "    ## Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "    ## Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "\n",
    "    ## Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "\n",
    "    ## Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "\n",
    "    ## Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        ## Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        ## Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "\n",
    "        ## Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective='binary',\n",
    "                                   class_weight='balanced', learning_rate=0.05,\n",
    "                                   reg_alpha=0.1, reg_lambda=0.1,\n",
    "                                   subsample=0.8, n_jobs=-1, random_state=50)\n",
    "        \n",
    "        ## Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric='auc',\n",
    "                  eval_set=[(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names=['valid', 'train'], categorical_feature=cat_indices,\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True)])\n",
    "                #   early_stopping_rounds=100, verbose=200)\n",
    "                #   인자가 callbacks=list로 바뀜\n",
    "        \n",
    "        ## Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "\n",
    "        ## Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "\n",
    "        ## Make prdeictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration=best_iteration)[:, 1] / k_fold.n_splits\n",
    "\n",
    "        ## Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration=best_iteration)[:, 1]\n",
    "\n",
    "        ## Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "\n",
    "        ## Record the best score\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        ## Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "\n",
    "    ## Make the submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "\n",
    "    ## Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "    ## Overall validation score\n",
    "    valid_acu = roc_auc_score(labels, out_of_fold)\n",
    "\n",
    "    ## Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_acu)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "\n",
    "    ## Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "\n",
    "    ## Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores})\n",
    "    \n",
    "    return submission, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission, fi, metrics = model(app_train, app_test)\n",
    "print('Baseline metrics')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_sorted = plot_feature_importances(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission.to_csv('baseline_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_domain['TARGET'] = train_labels\n",
    "\n",
    "# Test the domain knolwedge features\n",
    "submission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\n",
    "print('Baseline with domain knowledge features metrics')\n",
    "print(metrics_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_sorted = plot_feature_importances(fi_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_domain.to_csv('baselline_lgb_domain_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
